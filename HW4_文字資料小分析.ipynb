{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5vXkMmDazc4syNn/bX0UA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371120h/PL-Repo.peng/blob/main/HW4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ğŸ”¹ Yahoo è‚¡å¸‚æ–°èåˆ†æ â†’ TF-IDF â†’ Gemini AI æ´å¯Ÿ (Sheet å¼·åŒ–ç‰ˆ)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- é‹è¡Œç’°å¢ƒè¨­å®šï¼ˆè«‹åœ¨ Colab Cell ä¸­åŸ·è¡Œï¼‰---\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "              gradio pandas beautifulsoup4 google-generativeai python-dateutil scikit-learn jieba\n",
        "\n",
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from requests.exceptions import RequestException, Timeout\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "import jieba  # ä½¿ç”¨é‡å°ç¹é«”ä¸­æ–‡å„ªåŒ–çš„ jieba\n",
        "import jieba.analyse\n",
        "import jieba.posseg as pseg\n",
        "\n",
        "# Google Auth & Sheets\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "\n",
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import traceback\n",
        "import pytz\n",
        "\n",
        "# --- Google èªè­‰èˆ‡ Gemini é…ç½® ---\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    print(\"âœ… Google Sheets æˆæ¬ŠæˆåŠŸã€‚\")\n",
        "\n",
        "    GEMINI_API_KEY = userdata.get(\"gemini\")\n",
        "    if not GEMINI_API_KEY:\n",
        "         raise ValueError(\"Colab Secret 'gemini' is empty or not found. Please set your Gemini API Key.\")\n",
        "\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    model = genai.GenerativeModel(\"gemini-2.5-flash\") # ä½¿ç”¨æ›´å¿«é€Ÿçš„æ¨¡å‹\n",
        "    print(\"âœ… Gemini API Key é…ç½®æˆåŠŸã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸš¨ æˆæ¬Šæˆ–è¨­å®šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. å…¨åŸŸè®Šæ•¸èˆ‡ Sheet/DataFrame è¨­ç½®\n",
        "# ==============================================================================\n",
        "# è«‹æª¢æŸ¥æ‚¨çš„ Sheet URLï¼Œç¢ºä¿æ­£ç¢º\n",
        "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/107FcjXEnPn7vM10qFPj-wQFPeeNUOWTwKk5A-ejJqo4/edit?gid=412911623#gid=412911623\"\n",
        "TIMEZONE = \"Asia/Taipei\"\n",
        "\n",
        "# ç¢ºä¿é€™äº›æ¬„ä½èˆ‡ DF è¼¸å‡ºä¸€è‡´\n",
        "CLIPS_HEADER = [\"æ—¥æœŸ\", \"ä½œè€…\", \"æ¨™é¡Œ\", \"é€£çµ\", \"å…§æ–‡\"]\n",
        "STATS_HEADER = [\"é—œéµå­—\", \"TF-IDFå¹³å‡æ¬Šé‡\"]\n",
        "SUMMARY_HEADER = [\"created_at\", \"keywords_used\", \"summary_report\"]\n",
        "\n",
        "# --- Google Sheet åˆå§‹åŒ–å‡½å¼ (æ²¿ç”¨æ‚¨æä¾›çš„é‚è¼¯) ---\n",
        "def get_or_create_worksheet(sheet, title):\n",
        "    try:\n",
        "        worksheet = sheet.worksheet(title)\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        worksheet = sheet.add_worksheet(title=title, rows=\"100\", cols=\"20\")\n",
        "    return worksheet\n",
        "\n",
        "def write_to_sheet(sheet, worksheet_name, df, log_output, header_list):\n",
        "    log_output.append(f\"--- 2. Google Sheet å¯«å…¥æ—¥èªŒ ---\")\n",
        "    try:\n",
        "        worksheet = get_or_create_worksheet(sheet, worksheet_name)\n",
        "        # ğŸš¨ é—œéµä¿®æ­£ï¼šç¢ºä¿å¯«å…¥çš„æ¬„ä½èˆ‡ header_list ä¸€è‡´\n",
        "        if not df.empty:\n",
        "            df_to_write = df.reindex(columns=header_list, fill_value=\"\")\n",
        "\n",
        "            # ä½¿ç”¨ update å¯«å…¥æ¨™é¡Œå’Œæ•¸æ“š\n",
        "            worksheet.clear()\n",
        "            worksheet.update(\n",
        "                [df_to_write.columns.values.tolist()] + df_to_write.astype(str).values.tolist(),\n",
        "                value_input_option=\"USER_ENTERED\"\n",
        "            )\n",
        "            log_output.append(f\"âœ… æˆåŠŸå¯«å…¥ {worksheet_name} å·¥ä½œè¡¨ ({len(df_to_write)} ç­†è³‡æ–™)ã€‚\")\n",
        "        else:\n",
        "             worksheet.clear()\n",
        "             worksheet.update([header_list], value_input_option=\"USER_ENTERED\")\n",
        "             log_output.append(f\"âœ… {worksheet_name} å·¥ä½œè¡¨å·²æ¸…ç©º (ç„¡è³‡æ–™å¯«å…¥)ã€‚\")\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"âŒ å¯«å…¥ Sheet å¤±æ•—: {e}\")\n",
        "    return log_output\n",
        "\n",
        "# é–‹å•Ÿè©¦ç®—è¡¨ä¸¦åˆå§‹åŒ–å·¥ä½œè¡¨\n",
        "try:\n",
        "    gsheets = gc.open_by_url(SPREADSHEET_URL)\n",
        "    print(f\"âœ… æˆåŠŸé–‹å•Ÿ Sheet: {gsheets.title}\")\n",
        "    # é€™è£¡åªç¢ºä¿å·¥ä½œè¡¨å­˜åœ¨ï¼Œå¯¦éš›å¯«å…¥é‚è¼¯åœ¨ write_to_sheet ä¸­\n",
        "    ws_clips = get_or_create_worksheet(gsheets, \"Yahooæ–‡ç« åˆ—è¡¨\")\n",
        "    ws_stats = get_or_create_worksheet(gsheets, \"ç†±è©çµ±è¨ˆ\")\n",
        "    ws_summary = get_or_create_worksheet(gsheets, \"AIæ‘˜è¦å ±å‘Š\")\n",
        "except Exception as e:\n",
        "     print(f\"âŒ ç„¡æ³•åˆå§‹åŒ– Google Sheet: {e}\")\n",
        "     raise # åœæ­¢åŸ·è¡Œ\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. çˆ¬èŸ²ã€TF-IDF çµ±è¨ˆèˆ‡ Gemini æ‘˜è¦\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 2.1 Yahoo æ–°èçˆ¬èŸ² (æ ¸å¿ƒä¿®æ­£) ---\n",
        "YAHOO_STOCK_URL = \"https://tw.stock.yahoo.com/news\"\n",
        "\n",
        "def scrape_yahoo_stock_news(num_articles_to_fetch, log_output):\n",
        "    \"\"\"å°ˆé–€çˆ¬å– Yahoo è‚¡å¸‚æ–°èæŒ‡å®šæ–‡ç« æ•¸çš„æ–‡ç« åˆ—è¡¨èˆ‡å…§æ–‡\"\"\"\n",
        "\n",
        "    # ä½¿ç”¨é€šç”¨çš„æ–‡ç« åˆ—è¡¨ selector\n",
        "    LIST_SELECTOR = \"a[href*='tw.stock.yahoo.com/news/']\"\n",
        "\n",
        "    session = requests.Session()\n",
        "    all_data_list = []\n",
        "    log_output.append(f\"--- 1. çˆ¬èŸ²æ—¥èªŒ ---\")\n",
        "    log_output.append(f\"ç›®æ¨™ç¶²ç«™: Yahoo è‚¡å¸‚æ–°è | çˆ¬å–æ–‡ç« æ•¸: {num_articles_to_fetch}\")\n",
        "\n",
        "    # ä½¿ç”¨æ¨™æº– Headers æ¨¡æ“¬ç€è¦½å™¨\n",
        "    enhanced_headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "        \"Accept-Language\": \"zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # 1. çˆ¬å–åˆ—è¡¨é \n",
        "        r = session.get(YAHOO_STOCK_URL, timeout=15, headers=enhanced_headers)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        article_links = soup.select(LIST_SELECTOR)[:num_articles_to_fetch]\n",
        "\n",
        "        log_output.append(f\"åˆ—è¡¨é æ‰¾åˆ° {len(article_links)} ç¯‡æ–‡ç« é€£çµã€‚\")\n",
        "\n",
        "        # 2. çˆ¬å–å…§é \n",
        "        for i, a_tag in enumerate(article_links):\n",
        "            link = a_tag.get(\"href\")\n",
        "            if not link or \"javascript:void(0)\" in link: continue\n",
        "\n",
        "            # è™•ç†ç›¸å°è·¯å¾‘\n",
        "            if not link.startswith(\"http\"):\n",
        "                from urllib.parse import urljoin\n",
        "                link = urljoin(YAHOO_STOCK_URL, link)\n",
        "\n",
        "            # çˆ¬å–å…§é \n",
        "            try:\n",
        "                sub_resp = session.get(link, timeout=10, headers=enhanced_headers)\n",
        "                sub_resp.raise_for_status()\n",
        "                sub_soup = BeautifulSoup(sub_resp.text, \"html.parser\")\n",
        "\n",
        "                # æŠ“å–æ¨™é¡Œ (h1)\n",
        "                title = sub_soup.select_one(\"h1\").get_text(strip=True) if sub_soup.select_one(\"h1\") else \"ç„¡æ¨™é¡Œ\"\n",
        "\n",
        "                # æŠ“å–å…§å®¹ (p æ¨™ç±¤å…§æ–‡)\n",
        "                content_nodes = sub_soup.select(\"p\")\n",
        "                content = \" \".join([p.get_text(strip=True) for p in content_nodes if len(p.get_text(strip=True)) > 20])\n",
        "\n",
        "                # æŠ“å–ä½œè€…/æ—¥æœŸ (é€šå¸¸åœ¨ç‰¹å®šçš„ span/div å…§ï¼Œé€™è£¡ä½¿ç”¨ç°¡åŒ–æ–¹å¼)\n",
        "                date_node = sub_soup.select_one(\"time\")\n",
        "                date_str = date_node.get(\"datetime\") if date_node and date_node.get(\"datetime\") else dt.now(gettz(TIMEZONE)).strftime(\"%m/%d\")\n",
        "                author = sub_soup.select_one(\"span.author-name\")\n",
        "                author_str = author.get_text(strip=True) if author else \"Yahoo è‚¡å¸‚\"\n",
        "\n",
        "                all_data_list.append({\n",
        "                    \"æ—¥æœŸ\": date_str,\n",
        "                    \"ä½œè€…\": author_str,\n",
        "                    \"æ¨™é¡Œ\": title,\n",
        "                    \"é€£çµ\": link,\n",
        "                    \"å…§æ–‡\": content\n",
        "                })\n",
        "                log_output.append(f\"   -> æˆåŠŸæ“·å– #{i+1}: {title[:20]}...\")\n",
        "            except Exception as e:\n",
        "                log_output.append(f\"   âš ï¸ çˆ¬å–æˆ–è§£æå…§é å¤±æ•— ({link}): {e}\")\n",
        "                continue\n",
        "\n",
        "            time.sleep(0.1)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        log_output.append(f\"âŒ çˆ¬èŸ²èµ·å§‹è«‹æ±‚å¤±æ•—ï¼š{e}\")\n",
        "\n",
        "    df = pd.DataFrame(all_data_list)\n",
        "    log_output.append(f\"âœ… çˆ¬èŸ²çµæŸã€‚å…±æŠ“å– {len(df)} ç¯‡æ–‡ç« ã€‚\")\n",
        "    return df, log_output\n",
        "\n",
        "# --- 2.2 TF-IDF é—œéµå­—åˆ†æ (ä¸­æ–‡é‡‘èå„ªåŒ–) ---\n",
        "\n",
        "STOPWORDS = set([\n",
        "    'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'ä¹‹', 'ä¸€å€‹', 'å’Œ', 'èˆ‡', 'æˆ–', 'ä¹Ÿ', 'éƒ½', 'å°‡',\n",
        "    'è¢«', 'ç”±', 'æ‰€', 'æ–¼', 'æ–¼æ­¤', 'é€™', 'é‚£', 'è€Œ', 'ä½†', 'ä¸¦', 'å‰‡', 'è¦', 'æ‡‰', 'é€²è¡Œ', 'å¦‚æœ',\n",
        "    # æ•¸å­—èˆ‡å–®ä½\n",
        "    'å…ƒ', 'è¬å…ƒ', 'å„„å…ƒ', 'è¬', 'å„„', 'åƒ', 'ç™¾', 'å€‹', 'æ—¥', 'æœˆ', 'å¹´', 'å­£', 'é€±', 'å¤©', 'é»', 'åº¦',\n",
        "    # å¸¸ç”¨è©\n",
        "    'å…¬å¸', 'ä¼æ¥­', 'å¸‚å ´', 'æŒ‡å‡º', 'è¡¨ç¤º', 'å ±å°', 'åˆ†æ', 'èªç‚º', 'æä¾›', 'è³‡è¨Š', 'ç¶²ç«™', 'è‚¡åƒ¹', 'è‚¡å¸‚',\n",
        "    'æŠ•è³‡', 'äº¤æ˜“', 'å®¢æˆ¶', 'æ¥­å‹™', 'ç”¢å“', 'æœå‹™', 'è³‡æ–™', 'å·²ç¶“', 'ä¸é', 'æ­¤å¤–', 'ç›®å‰', 'æœªä¾†', 'é è¨ˆ',\n",
        "    # ä»‹ç³»è©èˆ‡é€£è©\n",
        "    'å°æ–¼', 'é—œæ–¼', 'ç”±æ–¼', 'å› ç‚º', 'éš¨è‘—', 'é™¤äº†', 'åŒ…æ‹¬', 'ä¾‹å¦‚', 'å¦‚æœèªª', 'ç”šè‡³', 'é‚„æ˜¯', 'é‚„æ˜¯èªª'\n",
        "])\n",
        "\n",
        "def chinese_tokenizer(text):\n",
        "    \"\"\"åˆ†è©ä¸¦éæ¿¾åœç”¨è©å’Œå–®å­—\"\"\"\n",
        "    # ç§»é™¤æ¨™é»ç¬¦è™Ÿå’Œæ•¸å­—\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', ' ', text).strip()\n",
        "    cleaned_text = re.sub(r'\\d+', ' ', cleaned_text)\n",
        "\n",
        "    # ä½¿ç”¨ç²¾ç¢ºæ¨¡å¼åˆ†è©\n",
        "    words = jieba.lcut(cleaned_text, cut_all=False)\n",
        "\n",
        "    # éæ¿¾åœç”¨è©å’Œå–®å­—\n",
        "    filtered_words = [\n",
        "        word.strip()\n",
        "        for word in words\n",
        "        if word.strip() and len(word.strip()) > 1 and word.strip().lower() not in STOPWORDS\n",
        "    ]\n",
        "    return filtered_words\n",
        "\n",
        "\n",
        "def get_tfidf_keywords(df, top_n, log_output):\n",
        "    \"\"\"ä½¿ç”¨ sklearn.TfidfVectorizer é€²è¡Œ TF-IDF åˆ†æ\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 3. TF-IDF åˆ†ææ—¥èªŒ (Sklearn) ---\")\n",
        "    log_output.append(f\"ç›®æ¨™é—œéµå­—æ•¸é‡: Top {top_n}\")\n",
        "    log_output.append(f\"åœç”¨è©æ•¸é‡: {len(STOPWORDS)}\")\n",
        "\n",
        "    if 'å…§æ–‡' not in df.columns or df['å…§æ–‡'].dropna().empty:\n",
        "        log_output.append(\"âŒ éŒ¯èª¤: è³‡æ–™é›†ä¸­ç¼ºå°‘ 'å…§æ–‡' æ¬„ä½æˆ–å…§æ–‡ç‚ºç©ºã€‚\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "    document_list = []\n",
        "\n",
        "    # 1. æ–‡æœ¬é è™•ç†èˆ‡åˆ†è©\n",
        "    for content in df['å…§æ–‡'].dropna():\n",
        "        filtered_words = chinese_tokenizer(content)\n",
        "        if filtered_words:\n",
        "            document_list.append(\" \".join(filtered_words))\n",
        "\n",
        "    log_output.append(f\"å·²å°‡ {len(df)} ç¯‡æ–‡ç« åˆ†è©ä¸¦éæ¿¾ï¼Œç”¢ç”Ÿ {len(document_list)} ç¯‡æœ‰æ•ˆæ–‡æª”ã€‚\")\n",
        "\n",
        "    if not document_list:\n",
        "        log_output.append(\"âš ï¸ æ²’æœ‰å¯åˆ†æçš„æ–‡æª” (å¯èƒ½éƒ½è¢«éæ¿¾äº†)ã€‚\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "    # 2. TF-IDF è¨ˆç®—\n",
        "    try:\n",
        "        # ä½¿ç”¨è‡ªå®šç¾©åˆ†è©å™¨ï¼Œä¸¦è¨­å®š n-gram (1-gram å’Œ 2-gram)\n",
        "        vectorizer = TfidfVectorizer(tokenizer=chinese_tokenizer, ngram_range=(1, 2))\n",
        "        tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        log_output.append(f\"âœ… TF-IDF çŸ©é™£å»ºç«‹æˆåŠŸã€‚è©å½™ç¸½æ•¸ (Features): {len(feature_names)}\")\n",
        "\n",
        "        # 3. è¨ˆç®—å¹³å‡æ¬Šé‡ (æ‰¾å‡ºæ•´é«”é‡è¦æ€§)\n",
        "        # ğŸš¨ é—œéµä¿®æ­£: ä½¿ç”¨ sum(axis=0) è¨ˆç®—ç¸½æ¬Šé‡è€Œéå¹³å‡ï¼Œåœ¨ä¸åŒæ–‡ä»¶æ•¸é‡ä¸‹æ›´ç©©å®š\n",
        "        sum_tfidf_scores = tfidf_matrix.sum(axis=0).tolist()[0]\n",
        "\n",
        "        # 4. å»ºç«‹ DataFrame æ’åºä¸¦é¸å‡º Top N\n",
        "        keywords_with_scores = list(zip(feature_names, sum_tfidf_scores))\n",
        "\n",
        "        # æ’åº\n",
        "        sorted_keywords = sorted(keywords_with_scores, key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        top_keywords_df = pd.DataFrame(\n",
        "            [(k, round(s, 4)) for k, s in sorted_keywords[:top_n]],\n",
        "            columns=['é—œéµå­—', 'TF-IDFå¹³å‡æ¬Šé‡']\n",
        "        )\n",
        "\n",
        "        log_output.append(f\"âœ… æˆåŠŸæå– Top {len(top_keywords_df)} å€‹é—œéµå­—ã€‚\")\n",
        "        log_output.append(f\"Top 5 é—œéµå­—ç¯„ä¾‹: {', '.join(top_keywords_df['é—œéµå­—'].head(5).tolist())}\")\n",
        "\n",
        "        return top_keywords_df, log_output\n",
        "\n",
        "    except ValueError as e:\n",
        "        log_output.append(f\"âŒ TF-IDF åˆ†æå¤±æ•— (ValueError): {e}\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"âŒ TF-IDF åˆ†æç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "\n",
        "# --- 2.3 Gemini API ç”Ÿæˆæ‘˜è¦ (ä¿ç•™é«˜è¶…æ™‚è¨­å®š) ---\n",
        "def get_gemini_summary(keywords_df, log_output):\n",
        "    \"\"\"ä½¿ç”¨ Gemini API æ ¹æ“šé—œéµå­—ç”Ÿæˆæ‘˜è¦ï¼Œä¸¦å°‡çµæœå¯«å…¥ Sheet\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 4. Gemini æ‘˜è¦æ—¥èªŒ ---\")\n",
        "\n",
        "    if keywords_df.empty:\n",
        "        log_output.append(\"âš ï¸ ç¼ºå°‘é—œéµå­—ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦ã€‚\")\n",
        "        return \"âš ï¸ æ²’æœ‰é—œéµå­—ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦ã€‚\", log_output\n",
        "\n",
        "    keywords_list = keywords_df['é—œéµå­—'].tolist()\n",
        "    prompt = f\"\"\"\n",
        "    æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„è‚¡å¸‚æ•¸æ“šåˆ†æå¸«ã€‚\n",
        "\n",
        "    ä»»å‹™ï¼š\n",
        "    è«‹æ ¹æ“š Yahoo è‚¡å¸‚æ–°èçš„ {len(keywords_list)} å€‹ç†±é–€é—œéµå­—ï¼Œç”Ÿæˆä¸€ä»½å°ˆæ¥­çš„è‚¡å¸‚åˆ†æå ±å‘Šã€‚\n",
        "\n",
        "    ç†±é–€é—œéµå­— (ä¾ TF-IDF ç¸½æ¬Šé‡æ’åº)ï¼š\n",
        "    {', '.join(keywords_list)}\n",
        "\n",
        "    è¼¸å‡ºæ ¼å¼è¦æ±‚ (è«‹åš´æ ¼éµå®ˆ)ï¼š\n",
        "    1.  **äº”å¥æ´å¯Ÿæ‘˜è¦**ï¼šæ¢åˆ—å¼ï¼Œæ¯å¥éƒ½æ˜¯ç²¾é—¢çš„è‚¡å¸‚è§€å¯Ÿã€‚\n",
        "    2.  **ä¸€æ®µ 120 å­—çµè«–**ï¼šç¸½çµç›®å‰çš„è‚¡å¸‚è¶¨å‹¢æˆ–æŠ•è³‡æ©Ÿæœƒã€‚\n",
        "\n",
        "    è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\n",
        "    \"\"\"\n",
        "\n",
        "    summary_text = \"\"\n",
        "    try:\n",
        "        log_output.append(f\"æ¨¡å‹è«‹æ±‚åƒæ•¸: gemini-2.5-flash, é—œéµå­—æ•¸é‡: {len(keywords_list)}\")\n",
        "        # ğŸš¨ é—œéµä¿®æ­£ï¼šä¿æŒ 120 ç§’è¶…æ™‚ï¼Œæ‡‰å°é€£ç·šä¸ç©©å®šçš„å•é¡Œ\n",
        "        response = model.generate_content(prompt, request_options={\"timeout\": 120})\n",
        "\n",
        "        summary_text = response.text.replace(\"#\", \"\").replace(\"*\", \"\")\n",
        "        log_output.append(\"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸã€‚\")\n",
        "        msg_status = \"âœ… Gemini æ‘˜è¦ç”Ÿæˆå®Œæˆã€‚\"\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ Gemini API å‘¼å«å¤±æ•—ï¼š{e}\"\n",
        "        log_output.append(error_msg)\n",
        "        summary_text = error_msg\n",
        "        msg_status = f\"âš ï¸ Gemini è«‹æ±‚å¤±æ•—ï¼Œè©³æƒ…è«‹çœ‹æ—¥èªŒã€‚\"\n",
        "\n",
        "    # --- å¯«å…¥ AI æ‘˜è¦è‡³ Sheet ---\n",
        "    try:\n",
        "        # å°‡æœ€æ–°çš„æ‘˜è¦å¯«å…¥ 'AIæ‘˜è¦å ±å‘Š' å·¥ä½œè¡¨\n",
        "        new_row_df = pd.DataFrame([{\n",
        "            \"created_at\": dt.now(gettz(TIMEZONE)).isoformat(),\n",
        "            \"keywords_used\": \", \".join(keywords_list[:10]), # åªå¯«å…¥å‰ 10 å€‹é—œéµå­—\n",
        "            \"summary_report\": summary_text\n",
        "        }], columns=SUMMARY_HEADER)\n",
        "\n",
        "        # ç”±æ–¼æ‚¨åŸç¨‹å¼ä¸­æ²’æœ‰ read_df for summaryï¼Œé€™è£¡ä½¿ç”¨ gspread çš„ append å¯«å…¥ (æ”¾åœ¨æœ€å‰é¢éœ€è¦æ›´å¤šé‚è¼¯ï¼Œå…ˆç”¨å¯«å…¥å–®è¡Œæ›¿ä»£)\n",
        "        # ç¢ºä¿åœ¨ Sheet ä¸­ï¼Œæ­¤å·¥ä½œè¡¨ 'AIæ‘˜è¦å ±å‘Š' å·²ç¶“æœ‰ SUMMARY_HEADER\n",
        "\n",
        "        # ğŸš¨ ä¿®æ­£ï¼šä½¿ç”¨ df çš„å‰å¹¾è¡Œæ¨¡æ“¬å¯«å…¥é‚è¼¯\n",
        "        # è®€å–ç¾æœ‰æ•¸æ“šï¼Œå°‡æ–°æ•¸æ“šæ’å…¥æœ€å‰é¢ï¼Œç„¶å¾Œå¯«å› (ç¢ºä¿æœ€æ–°çš„åœ¨ä¸Šæ–¹)\n",
        "        df_existing = ws_summary.get_all_values()\n",
        "        df_existing_data = [row for row in df_existing if row != SUMMARY_HEADER]\n",
        "\n",
        "        # æº–å‚™æ–°è³‡æ–™åˆ— (å¿…é ˆæ˜¯ list of lists)\n",
        "        new_data_row = new_row_df.iloc[0].values.tolist()\n",
        "\n",
        "        # æ¸…ç©ºå·¥ä½œè¡¨ä¸¦å¯«å…¥æ–°çš„æ¨™é¡Œå’Œæ•¸æ“š\n",
        "        ws_summary.clear()\n",
        "        ws_summary.update([SUMMARY_HEADER] + [new_data_row] + df_existing_data, value_input_option=\"USER_ENTERED\")\n",
        "\n",
        "        log_output.append(\"âœ… æ‘˜è¦å’Œé—œéµè©å·²æˆåŠŸå¯«å…¥ 'AIæ‘˜è¦å ±å‘Š' å·¥ä½œè¡¨ã€‚\")\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"âŒ å¯«å…¥ AI æ‘˜è¦è‡³ Sheet å¤±æ•—: {e}\")\n",
        "\n",
        "    return summary_text, log_output\n",
        "\n",
        "# ================================\n",
        "# 3. Gradio æ•´åˆå‡½å¼\n",
        "# ================================\n",
        "def run_full_automation_flow(top_n_str, articles_to_fetch_str):\n",
        "    \"\"\"Gradio é»æ“Šå¾ŒåŸ·è¡Œçš„å®Œæ•´æµç¨‹\"\"\"\n",
        "\n",
        "    empty_df = pd.DataFrame(columns=STATS_HEADER)\n",
        "    empty_scraped_df = pd.DataFrame(columns=CLIPS_HEADER)\n",
        "    empty_str = \"\"\n",
        "    log_output = []\n",
        "\n",
        "    SITE_NAME = \"Yahoo è‚¡å¸‚æ–°è\"\n",
        "    site_list = [SITE_NAME]\n",
        "\n",
        "    # æ¸…ç©ºä¸Šæ¬¡çµæœ\n",
        "    yield \"æ—¥èªŒå°‡é¡¯ç¤ºæ–¼æ­¤...\", empty_df, empty_str, None, gr.Radio(choices=[\"å°šæœªåŸ·è¡Œ\"], value=\"å°šæœªåŸ·è¡Œ\"), empty_scraped_df\n",
        "\n",
        "    # --- åƒæ•¸é©—è­‰ ---\n",
        "    try:\n",
        "        top_n = int(top_n_str)\n",
        "        articles_to_fetch = int(articles_to_fetch_str)\n",
        "        if top_n <= 0 or articles_to_fetch <= 0:\n",
        "            log_output.append(\"âŒ Top N æˆ–çˆ¬å–æ–‡ç« æ•¸å¿…é ˆæ˜¯å¤§æ–¼ 0 çš„æ•¸å­—ã€‚\")\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), empty_scraped_df\n",
        "            return\n",
        "    except ValueError:\n",
        "        log_output.append(\"âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—ã€‚\")\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), empty_scraped_df\n",
        "        return\n",
        "\n",
        "    # --- è‡ªå‹•åŒ–æµç¨‹ ---\n",
        "    log_output.append(\"===================================================\")\n",
        "    current_time_str = dt.now(gettz(TIMEZONE)).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    log_output.append(f\"ğŸš€ è‡ªå‹•åŒ–æµç¨‹å•Ÿå‹• ({current_time_str})\")\n",
        "    log_output.append(\"===================================================\")\n",
        "\n",
        "    try:\n",
        "        # --- æ­¥é©Ÿ 1: çˆ¬èŸ² (Yahoo News) ---\n",
        "        log_output.append(f\"1/4: ğŸƒâ€â™‚ï¸ é–‹å§‹çˆ¬å– {SITE_NAME} æ–‡ç« ï¼Œè«‹ç¨ç­‰...\")\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), empty_scraped_df\n",
        "\n",
        "        scraped_df, log_output = scrape_yahoo_stock_news(articles_to_fetch, log_output)\n",
        "        display_df = scraped_df[[\"æ—¥æœŸ\", \"ä½œè€…\", \"æ¨™é¡Œ\", \"é€£çµ\", \"å…§æ–‡\"]] # ç¢ºä¿é¡¯ç¤ºæ­£ç¢ºæ¬„ä½\n",
        "\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "\n",
        "        if scraped_df.empty:\n",
        "            log_output.append(\"âŒ çˆ¬èŸ²å¤±æ•—ï¼ŒæœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™ã€‚æµç¨‹çµ‚æ­¢ã€‚\")\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), empty_scraped_df\n",
        "            return\n",
        "\n",
        "        # --- æ­¥é©Ÿ 2: å¯«å…¥ Sheet (æ–‡ç« åˆ—è¡¨) ---\n",
        "        log_output = write_to_sheet(gsheets, \"Yahooæ–‡ç« åˆ—è¡¨\", scraped_df, log_output, CLIPS_HEADER)\n",
        "\n",
        "        # --- æ­¥é©Ÿ 3: TF-IDF åˆ†æ ---\n",
        "        log_output.append(\"2/4: ğŸ“Š æ­£åœ¨é€²è¡Œ Sklearn TF-IDF é—œéµå­—åˆ†æ...\")\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "\n",
        "        keywords_df, log_output = get_tfidf_keywords(scraped_df, top_n, log_output)\n",
        "\n",
        "        plot_df = None\n",
        "        if not keywords_df.empty:\n",
        "          # ç‚ºäº†è®“ BarPlot é ‚éƒ¨é¡¯ç¤ºæ¬Šé‡æœ€é«˜çš„ï¼Œæˆ‘å€‘éœ€å°‡ df å‡å†ªæ’åº\n",
        "          plot_df = keywords_df.sort_values(\"TF-IDFå¹³å‡æ¬Šé‡\", ascending=True)\n",
        "\n",
        "        yield \"\\n\".join(log_output), keywords_df, empty_str, plot_df, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "\n",
        "        if keywords_df.empty:\n",
        "            log_output.append(\"âš ï¸ åˆ†æå®Œæˆï¼Œä½†æœªæå–åˆ°é—œéµå­—ã€‚æµç¨‹çµ‚æ­¢ã€‚\")\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "            return\n",
        "\n",
        "        # --- æ­¥é©Ÿ 4: å¯«å…¥ Sheet (ç†±è©çµ±è¨ˆ) ---\n",
        "        log_output.append(\"3/4: ğŸ“ˆ æ­£åœ¨å°‡ Top ç†±è©å›å¯«è‡³ Sheet (ç†±è©çµ±è¨ˆ)...\")\n",
        "        log_output = write_to_sheet(gsheets, \"ç†±è©çµ±è¨ˆ\", keywords_df, log_output, STATS_HEADER)\n",
        "\n",
        "        # --- æ­¥é©Ÿ 5: Gemini æ‘˜è¦èˆ‡å¯«å…¥ Sheet (AIæ‘˜è¦å ±å‘Š) ---\n",
        "        log_output.append(\"4/4: ğŸ§  æ­£åœ¨å‘¼å« Gemini API ç”Ÿæˆæ‘˜è¦ (å·²è¨­å®š 120s è¶…æ™‚)...\")\n",
        "        yield \"\\n\".join(log_output), keywords_df, empty_str, plot_df, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "\n",
        "        summary, log_output = get_gemini_summary(keywords_df, log_output)\n",
        "\n",
        "        # æœ€çµ‚å›å‚³\n",
        "        final_log = \"\\n\".join(log_output)\n",
        "        yield final_log, keywords_df, summary, plot_df, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "\n",
        "        log_output.append(\"===================================================\")\n",
        "        log_output.append(\"âœ… å…¨éƒ¨æµç¨‹å®Œæˆï¼è«‹åˆ‡æ›åˆ°ã€Œæœ€çµ‚çµæœã€æ¨™ç±¤é æŸ¥çœ‹ã€‚\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ æµç¨‹ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ï¼š{e}\\n{traceback.format_exc()}\"\n",
        "        log_output.append(error_msg)\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), empty_scraped_df\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4. å•Ÿå‹• Gradio ä»‹é¢ (å·²ä¿®æ”¹ç‚º Yahoo)\n",
        "# ================================\n",
        "print(\"\\nğŸš€ æ­£åœ¨å•Ÿå‹• Gradio ä»‹é¢...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"orange\"), title=\"Yahoo è‚¡å¸‚æ–°èåˆ†æèˆ‡ AI æ‘˜è¦ï¼ˆSheet å¼·åŒ–ç‰ˆï¼‰\") as demo:\n",
        "    gr.Markdown(\n",
        "\n",
        "        \"\"\"\n",
        "        # ğŸ“ˆ Yahoo è‚¡å¸‚æ–°èåˆ†æ â†’ TF-IDF é—œéµè© â†’ AI æ´å¯Ÿæ‘˜è¦\n",
        "        æ­¤å·¥å…·æœƒè‡ªå‹•åŸ·è¡Œï¼š**Yahoo çˆ¬èŸ² â†’ å¯«å…¥ Sheet (æ–‡ç« åˆ—è¡¨) â†’ TF-IDF çµ±è¨ˆ â†’ å¯«å…¥ Sheet (ç†±è©çµ±è¨ˆ) â†’ Gemini ç”Ÿæˆæ‘˜è¦ â†’ å¯«å…¥ Sheet (AIæ‘˜è¦å ±å‘Š)**ã€‚\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"ğŸš€ è‡ªå‹•åŒ–æµç¨‹åŸ·è¡Œ\"):\n",
        "        with gr.Row():\n",
        "            # ğŸš¨ ä¿®æ­£ï¼šåƒæ•¸åç¨±å¾ pages_to_fetch æ”¹ç‚º articles_to_fetch\n",
        "            articles_to_fetch_input = gr.Textbox(label=\"è¦çˆ¬å–çš„æ–‡ç« æ•¸é‡ (Limit)\", value=\"10\", scale=1)\n",
        "            top_n_input = gr.Textbox(label=\"è¦çµ±è¨ˆçš„ Top N ç†±è©æ•¸é‡\", value=\"20\", scale=1)\n",
        "            run_btn = gr.Button(\"ğŸš€ ä¸€éµå•Ÿå‹• Yahoo è‚¡å¸‚æ–°èåˆ†æ\", variant=\"primary\", scale=2)\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        # é¡¯ç¤º Yahoo çˆ¬èŸ²çš„åƒæ•¸è³‡è¨Š\n",
        "        with gr.Row():\n",
        "            gr.Textbox(label=\"ç›®æ¨™ç¶²ç«™\", value=YAHOO_STOCK_URL, interactive=False, scale=1)\n",
        "            gr.Textbox(label=\"çˆ¬èŸ²æ¨¡å¼\", value=\"å°ˆé–€é‡å° Yahoo å…§é æ“·å–\", interactive=False, scale=1)\n",
        "\n",
        "        # ä½¿ç”¨ Tab ä¾†å€åˆ†æœ€çµ‚çµæœå’Œæ—¥èªŒ\n",
        "        with gr.Tabs():\n",
        "\n",
        "            with gr.TabItem(\"ğŸ› ï¸ æŠ€è¡“æ—¥èªŒèˆ‡è¼¸å‡ºç´°ç¯€\"):\n",
        "                log_output_text = gr.Textbox(\n",
        "                    label=\"è©³ç´°æµç¨‹æ—¥èªŒ (çˆ¬èŸ²ã€å¯«å…¥ã€åˆ†ææ­¥é©Ÿ)\",\n",
        "                    lines=30,\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "            with gr.TabItem(\"ğŸ•¸ï¸ çˆ¬å–æ–‡ç« åˆ—è¡¨\"):\n",
        "                site_list_output = gr.Radio(\n",
        "                    label=\"è³‡æ–™ä¾†æº\",\n",
        "                    choices=[\"å°šæœªåŸ·è¡Œ\"],\n",
        "                    value=\"å°šæœªåŸ·è¡Œ\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                gr.Markdown(\"---\")\n",
        "\n",
        "                # ç”±æ–¼çˆ¬èŸ²é‚è¼¯å·²æ”¹ï¼Œé€™è£¡åªé¡¯ç¤º DataFrameï¼Œé»æ“Šé€£çµåŠŸèƒ½æš«ä¸å¯¦ä½œ\n",
        "                scraped_data_output = gr.Dataframe(\n",
        "                    label=\"çˆ¬å–æ–‡ç« åˆ—è¡¨ (åŸå§‹è³‡æ–™)\",\n",
        "                    headers=[\"æ—¥æœŸ\", \"ä½œè€…\", \"æ¨™é¡Œ\", \"é€£çµ\", \"å…§æ–‡\"],\n",
        "                    interactive=True,\n",
        "                    row_count=(15, 'dynamic')\n",
        "                )\n",
        "\n",
        "                # é‡æ–°ç¶å®š Dataframe é»æ“Šäº‹ä»¶ï¼Œç¢ºä¿å³ä½¿é€£çµåŠŸèƒ½æœªå¯¦ä½œï¼Œä¹Ÿä¸æœƒå ±éŒ¯\n",
        "                link_display_output = gr.Markdown(\n",
        "          \t\t\tvalue=\"*åŸå§‹æ–‡ç« è³‡æ–™å·²é¡¯ç¤ºæ–¼è¡¨æ ¼ã€‚*\"\n",
        "          \t\t)\n",
        "                # ç§»é™¤ show_selected_link é»æ“Šäº‹ä»¶ï¼Œé¿å… Gradio éŒ¯èª¤\n",
        "\n",
        "            with gr.TabItem(\"âœ… æœ€çµ‚çµæœ\"):\n",
        "                summary_output = gr.Markdown(label=\"ğŸ¤– Gemini æ´å¯Ÿæ‘˜è¦èˆ‡çµè«–\")\n",
        "\n",
        "                keyword_plot_output = gr.BarPlot(\n",
        "                  label=\"ğŸ“ˆ Top N ç†±è©è¦–è¦ºåŒ–åœ–è¡¨\",\n",
        "                  x=\"TF-IDFå¹³å‡æ¬Šé‡\",\n",
        "                  y=\"é—œéµå­—\",\n",
        "                  tooltip=['é—œéµå­—', 'TF-IDFå¹³å‡æ¬Šé‡'],\n",
        "                  color=\"TF-IDFå¹³å‡æ¬Šé‡\",\n",
        "                  vertical=False,\n",
        "                  height=400\n",
        "                )\n",
        "\n",
        "                keywords_output = gr.Dataframe(label=\"ğŸ“ˆ Top N ç†±è©çµ±è¨ˆçµæœ (Sklearn TF-IDF ç¸½æ¬Šé‡)\")\n",
        "\n",
        "\n",
        "        # === ç¶å®šå‹•ä½œ ===\n",
        "        run_btn.click(\n",
        "          fn=run_full_automation_flow,\n",
        "          \tinputs=[top_n_input, articles_to_fetch_input],\n",
        "          \toutputs=[\n",
        "                log_output_text,\n",
        "                keywords_output,\n",
        "                summary_output,\n",
        "                keyword_plot_output,\n",
        "                site_list_output,\n",
        "                scraped_data_output\n",
        "            ]\n",
        "        )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "PXpEZsmjx4Yw",
        "outputId": "cb795c5f-c22f-4f39-878a-aea0608bc001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Google Sheets æˆæ¬ŠæˆåŠŸã€‚\n",
            "âœ… Gemini API Key é…ç½®æˆåŠŸã€‚\n",
            "âœ… æˆåŠŸé–‹å•Ÿ Sheet: HW4_æ–‡å­—è³‡æ–™å°åˆ†æ\n",
            "\n",
            "ğŸš€ æ­£åœ¨å•Ÿå‹• Gradio ä»‹é¢...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://228a41780666910844.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://228a41780666910844.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}