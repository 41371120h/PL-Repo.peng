{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy8GI/4qveKs7qn2Eq7Io+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371120h/PL-Repo.peng/blob/main/HW4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#試算表連結：https://docs.google.com/spreadsheets/d/107FcjXEnPn7vM10qFPj-wQFPeeNUOWTwKk5A-ejJqo4/edit?gid=616863925#gid=616863925"
      ],
      "metadata": {
        "id": "Msjc1h4KQj-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 🔹 Yahoo 股市新聞分析 → TF-IDF → Gemini AI 洞察 (最終穩定版 V4)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 運行環境設定（請在 Colab Cell 中執行）---\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "              gradio pandas beautifulsoup4 google-generativeai python-dateutil scikit-learn jieba\n",
        "\n",
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from requests.exceptions import RequestException, Timeout\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "import jieba.posseg as pseg\n",
        "\n",
        "# Google Auth & Sheets\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "\n",
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import traceback\n",
        "import pytz\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. 關鍵初始化區塊 (確保此區塊成功執行)\n",
        "# ==============================================================================\n",
        "gc = None\n",
        "gsheets = None\n",
        "gemini_model = None\n",
        "\n",
        "# 請檢查您的 Sheet URL，確保正確\n",
        "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/107FcjXEnPn7vM10qFPj-wQFPeeNUOWTwKk5A-ejJqo4/edit?gid=616863925#gid=616863925\"\n",
        "TIMEZONE = \"Asia/Taipei\"\n",
        "\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    print(\"✅ Google Sheets 授權成功。\")\n",
        "\n",
        "    GEMINI_API_KEY = userdata.get(\"gemini\")\n",
        "    if not GEMINI_API_KEY:\n",
        "         raise ValueError(\"Colab Secret 'gemini' is empty or not found. Please set your Gemini API Key.\")\n",
        "\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    gemini_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "    print(\"✅ Gemini API Key 配置成功。\")\n",
        "except Exception as e:\n",
        "    print(f\"🚨 授權或設定時發生錯誤：{e}\")\n",
        "    gsheets = None\n",
        "\n",
        "# 確保這些欄位與 DF 輸出一致\n",
        "CLIPS_HEADER = [\"日期\", \"作者\", \"標題\", \"連結\", \"內文\"]\n",
        "STATS_HEADER = [\"關鍵字\", \"TF-IDF平均權重\"]\n",
        "SUMMARY_HEADER = [\"created_at\", \"keywords_used\", \"summary_report\"]\n",
        "\n",
        "# ... (省略 1. Google Sheet 設置 和 2.1 Yahoo 新聞爬蟲，與 V3 版相同) ...\n",
        "# ==============================================================================\n",
        "# 1. Google Sheet 設置\n",
        "# ==============================================================================\n",
        "def get_or_create_worksheet(sheet, title):\n",
        "    try:\n",
        "        worksheet = sheet.worksheet(title)\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        worksheet = sheet.add_worksheet(title=title, rows=\"100\", cols=\"20\")\n",
        "    return worksheet\n",
        "\n",
        "def write_to_sheet(sheet, worksheet_name, df, log_output, header_list):\n",
        "    log_output.append(f\"--- 2. Google Sheet 寫入日誌 ---\")\n",
        "\n",
        "    if sheet is None:\n",
        "        log_output.append(\"❌ Google Sheet 連線失敗，跳過寫入。\")\n",
        "        return log_output\n",
        "\n",
        "    try:\n",
        "        worksheet = get_or_create_worksheet(sheet, worksheet_name)\n",
        "        if not df.empty:\n",
        "            df_to_write = df.reindex(columns=header_list, fill_value=\"\")\n",
        "\n",
        "            worksheet.clear()\n",
        "            worksheet.update(\n",
        "                [df_to_write.columns.values.tolist()] + df_to_write.astype(str).values.tolist(),\n",
        "                value_input_option=\"USER_ENTERED\"\n",
        "            )\n",
        "            log_output.append(f\"✅ 成功寫入 {worksheet_name} 工作表 ({len(df_to_write)} 筆資料)。\")\n",
        "        else:\n",
        "             worksheet.clear()\n",
        "             worksheet.update([header_list], value_input_option=\"USER_ENTERED\")\n",
        "             log_output.append(f\"✅ {worksheet_name} 工作表已清空 (無資料寫入)。\")\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"❌ 寫入 Sheet 失敗: {e}\")\n",
        "    return log_output\n",
        "\n",
        "# 開啟試算表並初始化工作表\n",
        "ws_summary = None\n",
        "if gsheets is None and gc:\n",
        "    try:\n",
        "        gsheets = gc.open_by_url(SPREADSHEET_URL)\n",
        "        print(f\"✅ 成功開啟 Sheet: {gsheets.title}\")\n",
        "        ws_clips = get_or_create_worksheet(gsheets, \"Yahoo文章列表\")\n",
        "        ws_stats = get_or_create_worksheet(gsheets, \"熱詞統計\")\n",
        "        ws_summary = get_or_create_worksheet(gsheets, \"AI摘要報告\")\n",
        "    except Exception as e:\n",
        "         print(f\"❌ 無法初始化 Google Sheet: {e}\")\n",
        "         gsheets = None\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. 爬蟲、TF-IDF 統計與 Gemini 摘要\n",
        "# ==============================================================================\n",
        "YAHOO_STOCK_URL = \"https://tw.stock.yahoo.com/news\"\n",
        "\n",
        "def scrape_yahoo_stock_news(num_articles_to_fetch, log_output):\n",
        "    \"\"\"專門爬取 Yahoo 股市新聞指定文章數的文章列表與內文\"\"\"\n",
        "\n",
        "    LIST_SELECTOR = \"a[href*='tw.stock.yahoo.com/news/']\"\n",
        "    session = requests.Session()\n",
        "    all_data_list = []\n",
        "    log_output.append(f\"--- 1. 爬蟲日誌 ---\")\n",
        "    log_output.append(f\"目標網站: Yahoo 股市新聞 | 爬取文章數: {num_articles_to_fetch}\")\n",
        "\n",
        "    enhanced_headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "        \"Accept-Language\": \"zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = session.get(YAHOO_STOCK_URL, timeout=15, headers=enhanced_headers)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        article_links = soup.select(LIST_SELECTOR)[:num_articles_to_fetch]\n",
        "\n",
        "        log_output.append(f\"列表頁找到 {len(article_links)} 篇文章連結。\")\n",
        "\n",
        "        for i, a_tag in enumerate(article_links):\n",
        "            link = a_tag.get(\"href\")\n",
        "            if not link or \"javascript:void(0)\" in link: continue\n",
        "\n",
        "            if not link.startswith(\"http\"):\n",
        "                from urllib.parse import urljoin\n",
        "                link = urljoin(YAHOO_STOCK_URL, link)\n",
        "\n",
        "            try:\n",
        "                sub_resp = session.get(link, timeout=10, headers=enhanced_headers)\n",
        "                sub_resp.raise_for_status()\n",
        "                sub_soup = BeautifulSoup(sub_resp.text, \"html.parser\")\n",
        "\n",
        "                title = sub_soup.select_one(\"h1\").get_text(strip=True) if sub_soup.select_one(\"h1\") else \"無標題\"\n",
        "\n",
        "                content_nodes = sub_soup.select(\"p\")\n",
        "                content = \" \".join([p.get_text(strip=True) for p in content_nodes if len(p.get_text(strip=True)) > 20])\n",
        "\n",
        "                date_node = sub_soup.select_one(\"time\")\n",
        "                date_str = date_node.get(\"datetime\") if date_node and date_node.get(\"datetime\") else dt.now(gettz(TIMEZONE)).strftime(\"%m/%d\")\n",
        "                author = sub_soup.select_one(\"span.author-name\")\n",
        "                author_str = author.get_text(strip=True) if author else \"Yahoo 股市\"\n",
        "\n",
        "                all_data_list.append({\n",
        "                    \"日期\": date_str,\n",
        "                    \"作者\": author_str,\n",
        "                    \"標題\": title,\n",
        "                    \"連結\": link,\n",
        "                    \"內文\": content\n",
        "                })\n",
        "                log_output.append(f\"   -> 成功擷取 #{i+1}: {title[:20]}...\")\n",
        "            except Exception as e:\n",
        "                log_output.append(f\"   ⚠️ 爬取或解析內頁失敗 ({link}): {e}\")\n",
        "                continue\n",
        "\n",
        "            time.sleep(0.1)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        log_output.append(f\"❌ 爬蟲起始請求失敗：{e}\")\n",
        "\n",
        "    df = pd.DataFrame(all_data_list)\n",
        "    log_output.append(f\"✅ 爬蟲結束。共抓取 {len(df)} 篇文章。\")\n",
        "    return df, log_output\n",
        "\n",
        "# --- 2.2 TF-IDF 關鍵字分析 ---\n",
        "STOPWORDS = set([\n",
        "    '的', '了', '是', '在', '我', '你', '他', '她', '之', '一個', '和', '與', '或', '也', '都', '將',\n",
        "    '被', '由', '所', '於', '於此', '這', '那', '而', '但', '並', '則', '要', '應', '進行', '如果',\n",
        "    '元', '萬元', '億元', '萬', '億', '千', '百', '個', '日', '月', '年', '季', '週', '天', '點', '度',\n",
        "    '公司', '企業', '市場', '指出', '表示', '報導', '分析', '認為', '提供', '資訊', '網站', '股價', '股市',\n",
        "    '投資', '交易', '客戶', '業務', '產品', '服務', '資料', '已經', '不過', '此外', '目前', '未來', '預計',\n",
        "    '對於', '關於', '由於', '因為', '隨著', '除了', '包括', '例如', '如果說', '甚至', '還是', '還是說'\n",
        "])\n",
        "\n",
        "def chinese_tokenizer(text):\n",
        "    \"\"\"分詞並過濾停用詞和單字\"\"\"\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', ' ', text).strip()\n",
        "    cleaned_text = re.sub(r'\\d+', ' ', cleaned_text)\n",
        "\n",
        "    words = jieba.lcut(cleaned_text, cut_all=False)\n",
        "\n",
        "    filtered_words = [\n",
        "        word.strip()\n",
        "        for word in words\n",
        "        if word.strip() and len(word.strip()) > 1 and word.strip().lower() not in STOPWORDS\n",
        "    ]\n",
        "    return filtered_words\n",
        "\n",
        "\n",
        "def get_tfidf_keywords(df, top_n, log_output):\n",
        "    \"\"\"使用 sklearn.TfidfVectorizer 進行 TF-IDF 分析\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 3. TF-IDF 分析日誌 (Sklearn) ---\")\n",
        "\n",
        "    if '內文' not in df.columns or df['內文'].dropna().empty:\n",
        "        log_output.append(\"❌ 錯誤: 資料集中缺少 '內文' 欄位或內文為空。\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "    document_list = []\n",
        "\n",
        "    for content in df['內文'].dropna():\n",
        "        filtered_words = chinese_tokenizer(content)\n",
        "        if filtered_words:\n",
        "            document_list.append(\" \".join(filtered_words))\n",
        "\n",
        "    if not document_list:\n",
        "        log_output.append(\"⚠️ 沒有可分析的文檔 (可能都被過濾了)。\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(tokenizer=chinese_tokenizer, ngram_range=(1, 2))\n",
        "        tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        sum_tfidf_scores = tfidf_matrix.sum(axis=0).tolist()[0]\n",
        "\n",
        "        keywords_with_scores = list(zip(feature_names, sum_tfidf_scores))\n",
        "\n",
        "        sorted_keywords = sorted(keywords_with_scores, key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        top_keywords_df = pd.DataFrame(\n",
        "            [(k, round(s, 4)) for k, s in sorted_keywords[:top_n]],\n",
        "            columns=['關鍵字', 'TF-IDF平均權重']\n",
        "        )\n",
        "\n",
        "        log_output.append(f\"✅ 成功提取 Top {len(top_keywords_df)} 個關鍵字。\")\n",
        "\n",
        "        return top_keywords_df, log_output\n",
        "\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"❌ TF-IDF 分析發生錯誤: {e}\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "\n",
        "# --- 2.3 Gemini API 生成摘要 (Model 傳遞強化與重試機制) ---\n",
        "def get_gemini_summary(keywords_df, log_output, model_obj):\n",
        "    \"\"\"接收 model_obj 作為參數，並新增重試機制\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 4. Gemini 摘要日誌 ---\")\n",
        "\n",
        "    if model_obj is None:\n",
        "        error_msg = \"❌ Gemini 模型未初始化。請確認 Colab Secret 'gemini' 已設定且授權成功。\"\n",
        "        log_output.append(error_msg)\n",
        "        return error_msg, log_output\n",
        "\n",
        "    if keywords_df.empty:\n",
        "        log_output.append(\"⚠️ 缺少關鍵字，無法生成摘要。\")\n",
        "        return \"⚠️ 沒有關鍵字，無法生成摘要。\", log_output\n",
        "\n",
        "    keywords_list = keywords_df['關鍵字'].tolist()\n",
        "    prompt = f\"\"\"\n",
        "    您是一位專業的股市數據分析師。\n",
        "\n",
        "    任務：\n",
        "    請根據 Yahoo 股市新聞的 {len(keywords_list)} 個熱門關鍵字，生成一份專業的股市分析報告。\n",
        "\n",
        "    熱門關鍵字 (依 TF-IDF 總權重排序)：\n",
        "    {', '.join(keywords_list)}\n",
        "\n",
        "    輸出格式要求 (請嚴格遵守)：\n",
        "    1.  **五句洞察摘要**：條列式，每句都是精闢的股市觀察。\n",
        "    2.  **一段 120 字結論**：總結目前的股市趨勢或投資機會。\n",
        "\n",
        "    請使用繁體中文回答。\n",
        "    \"\"\"\n",
        "\n",
        "    summary_text = \"\"\n",
        "    MAX_RETRIES = 5 # 保持 5 次重試\n",
        "    BASE_DELAY = 10\n",
        "    TIMEOUT_SECONDS = 180 # 保持 180 秒超時\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            log_output.append(f\"模型請求參數: gemini-2.5-flash, 嘗試次數: {attempt + 1}/{MAX_RETRIES}\")\n",
        "\n",
        "            # 使用傳入的 model_obj\n",
        "            response = model_obj.generate_content(prompt, request_options={\"timeout\": TIMEOUT_SECONDS})\n",
        "\n",
        "            summary_text = response.text.replace(\"#\", \"\").replace(\"*\", \"\")\n",
        "            log_output.append(\"✅ 摘要生成成功。\")\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"❌ Gemini API 呼叫失敗 (嘗試 {attempt + 1}): {e}\"\n",
        "            log_output.append(error_msg)\n",
        "\n",
        "            if attempt < MAX_RETRIES - 1:\n",
        "                sleep_time = BASE_DELAY * (attempt + 1)\n",
        "                log_output.append(f\"⏳ 等待 {sleep_time} 秒後重試...\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                summary_text = error_msg\n",
        "                log_output.append(\"❌ 重試次數已用盡，流程終止。\")\n",
        "\n",
        "    # --- 寫入 AI 摘要至 Sheet ---\n",
        "    global gsheets\n",
        "    if gsheets:\n",
        "        try:\n",
        "            ws_summary = get_or_create_worksheet(gsheets, \"AI摘要報告\")\n",
        "\n",
        "            new_row_df = pd.DataFrame([{\n",
        "                \"created_at\": dt.now(gettz(TIMEZONE)).isoformat(),\n",
        "                \"keywords_used\": \", \".join(keywords_list[:10]),\n",
        "                \"summary_report\": summary_text\n",
        "            }], columns=SUMMARY_HEADER)\n",
        "\n",
        "            df_existing = ws_summary.get_all_values()\n",
        "            df_existing_data = [row for row in df_existing if row != SUMMARY_HEADER]\n",
        "\n",
        "            new_data_row = new_row_df.iloc[0].values.tolist()\n",
        "\n",
        "            ws_summary.clear()\n",
        "            ws_summary.update([SUMMARY_HEADER] + [new_data_row] + df_existing_data, value_input_option=\"USER_ENTERED\")\n",
        "\n",
        "            log_output.append(\"✅ 摘要和關鍵詞已成功寫入 'AI摘要報告' 工作表。\")\n",
        "        except Exception as e:\n",
        "            log_output.append(f\"❌ 寫入 AI 摘要至 Sheet 失敗: {e}\")\n",
        "\n",
        "    return summary_text, log_output\n",
        "\n",
        "# ================================\n",
        "# 3. Gradio 整合函式 (使用單次回傳)\n",
        "# ================================\n",
        "def run_full_automation_flow(top_n_str, articles_to_fetch_str):\n",
        "    \"\"\"Gradio 點擊後執行的完整流程\"\"\"\n",
        "\n",
        "    global gsheets, gemini_model\n",
        "\n",
        "    empty_df = pd.DataFrame(columns=STATS_HEADER)\n",
        "    empty_scraped_df = pd.DataFrame(columns=CLIPS_HEADER)\n",
        "    empty_str = \"\"\n",
        "    log_output = []\n",
        "\n",
        "    SITE_NAME = \"Yahoo 股市新聞\"\n",
        "    site_list = [SITE_NAME]\n",
        "\n",
        "    # --- 參數初始化 (Gradio 要求回傳 6 個項目) ---\n",
        "    current_log = \"日誌將顯示於此...\"\n",
        "    current_keywords = empty_df\n",
        "    current_summary = empty_str\n",
        "    current_plot_df = None\n",
        "    current_site_radio = gr.Radio(choices=[\"尚未執行\"], value=\"尚未執行\")\n",
        "    current_scraped_data = empty_scraped_df\n",
        "\n",
        "\n",
        "    # --- 參數驗證 ---\n",
        "    try:\n",
        "        top_n = int(top_n_str)\n",
        "        articles_to_fetch = int(articles_to_fetch_str)\n",
        "        if top_n <= 0 or articles_to_fetch <= 0:\n",
        "            log_output.append(\"❌ Top N 或爬取文章數必須是大於 0 的數字。\")\n",
        "            current_log = \"\\n\".join(log_output)\n",
        "            return current_log, current_keywords, current_summary, current_plot_df, gr.Radio(choices=site_list), current_scraped_data\n",
        "    except ValueError:\n",
        "        log_output.append(\"❌ 請輸入有效的數字。\")\n",
        "        current_log = \"\\n\".join(log_output)\n",
        "        return current_log, current_keywords, current_summary, current_plot_df, gr.Radio(choices=site_list), current_scraped_data\n",
        "\n",
        "    # --- 關鍵檢查：Gemini Model 是否就緒 ---\n",
        "    if gemini_model is None:\n",
        "        log_output.append(\"❌ 流程終止：Gemini 模型未初始化。請確保 Colab Secret 'gemini' 已設定且授權成功。\")\n",
        "        current_log = \"\\n\".join(log_output)\n",
        "        return current_log, current_keywords, current_summary, current_plot_df, gr.Radio(choices=site_list, value=SITE_NAME), current_scraped_data\n",
        "\n",
        "    # --- 自動化流程 ---\n",
        "    log_output.append(\"===================================================\")\n",
        "    current_time_str = dt.now(gettz(TIMEZONE)).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    log_output.append(f\"🚀 自動化流程啟動 ({current_time_str})\")\n",
        "    log_output.append(\"===================================================\")\n",
        "\n",
        "    try:\n",
        "        # --- 步驟 1: 爬蟲 (Yahoo News) ---\n",
        "        log_output.append(f\"1/4: 🏃‍♂️ 開始爬取 {SITE_NAME} 文章，請稍等...\")\n",
        "\n",
        "        scraped_df, log_output = scrape_yahoo_stock_news(articles_to_fetch, log_output)\n",
        "        display_df = scraped_df[[\"日期\", \"作者\", \"標題\", \"連結\", \"內文\"]]\n",
        "        current_scraped_data = display_df\n",
        "        current_site_radio = gr.Radio(choices=site_list, value=SITE_NAME)\n",
        "\n",
        "        if scraped_df.empty:\n",
        "            log_output.append(\"❌ 爬蟲失敗，未抓取到任何資料。流程終止。\")\n",
        "            current_log = \"\\n\".join(log_output)\n",
        "            return current_log, current_keywords, current_summary, current_plot_df, current_site_radio, current_scraped_data\n",
        "\n",
        "        # --- 步驟 2: 寫入 Sheet (文章列表) ---\n",
        "        log_output = write_to_sheet(gsheets, \"Yahoo文章列表\", scraped_df, log_output, CLIPS_HEADER)\n",
        "\n",
        "        # --- 步驟 3: TF-IDF 分析 ---\n",
        "        log_output.append(\"2/4: 📊 正在進行 Sklearn TF-IDF 關鍵字分析...\")\n",
        "\n",
        "        keywords_df, log_output = get_tfidf_keywords(scraped_df, top_n, log_output)\n",
        "        current_keywords = keywords_df\n",
        "\n",
        "        if not keywords_df.empty:\n",
        "          current_plot_df = keywords_df.sort_values(\"TF-IDF平均權重\", ascending=True)\n",
        "\n",
        "        if keywords_df.empty:\n",
        "            log_output.append(\"⚠️ 分析完成，但未提取到關鍵字。流程終止。\")\n",
        "            current_log = \"\\n\".join(log_output)\n",
        "            return current_log, current_keywords, current_summary, current_plot_df, current_site_radio, current_scraped_data\n",
        "\n",
        "        # --- 步驟 4: 寫入 Sheet (熱詞統計) ---\n",
        "        log_output.append(\"3/4: 📈 正在將 Top 熱詞回寫至 Sheet (熱詞統計)...\")\n",
        "        log_output = write_to_sheet(gsheets, \"熱詞統計\", keywords_df, log_output, STATS_HEADER)\n",
        "\n",
        "        # --- 步驟 5: Gemini 摘要與寫入 Sheet (AI摘要報告) ---\n",
        "        log_output.append(\"4/4: 🧠 正在呼叫 Gemini API 生成摘要 (已設定 180s 超時, 5 次重試)...\")\n",
        "\n",
        "        # 將 gemini_model 物件作為參數傳遞\n",
        "        summary, log_output = get_gemini_summary(keywords_df, log_output, gemini_model)\n",
        "        current_summary = summary\n",
        "\n",
        "        # 最終回傳\n",
        "        final_log = \"\\n\".join(log_output)\n",
        "        log_output.append(\"===================================================\")\n",
        "        log_output.append(\"✅ 全部流程完成！請切換到「最終結果」標籤頁查看。\")\n",
        "\n",
        "        return final_log, current_keywords, current_summary, current_plot_df, current_site_radio, current_scraped_data\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ 流程發生未預期錯誤：{e}\\n{traceback.format_exc()}\"\n",
        "        log_output.append(error_msg)\n",
        "        final_log = \"\\n\".join(log_output)\n",
        "        return final_log, empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), empty_scraped_df\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4. 啟動 Gradio 介面 (使用橘色主題並強制公開分享)\n",
        "# ================================\n",
        "print(\"\\n🚀 正在啟動 Gradio 介面...\")\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"orange\"), title=\"Yahoo 股市新聞分析與 AI 摘要（Sheet 強化版）\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # 📈 Yahoo 股市新聞分析 → TF-IDF 關鍵詞 → AI 洞察摘要\n",
        "        此工具會自動執行：**Yahoo 爬蟲 → 寫入 Sheet (文章列表) → TF-IDF 統計 → 寫入 Sheet (熱詞統計) → Gemini 生成摘要 → 寫入 Sheet (AI摘要報告)**。\n",
        "        *為解決超時問題，已將 Gemini API 超時提高至 180 秒，並加入 5 次重試機制。請在 Gradio 提供的**公開網址** (Public URL) 上操作，以避免 Colab 內嵌介面的連線問題。*\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"🚀 自動化流程執行\"):\n",
        "        with gr.Row():\n",
        "            articles_to_fetch_input = gr.Textbox(label=\"要爬取的文章數量 (Limit)\", value=\"10\", scale=1)\n",
        "            top_n_input = gr.Textbox(label=\"要統計的 Top N 熱詞數量\", value=\"20\", scale=1)\n",
        "            run_btn = gr.Button(\"🚀 一鍵啟動 Yahoo 股市新聞分析\", variant=\"primary\", scale=2)\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        with gr.Row():\n",
        "            gr.Textbox(label=\"目標網站\", value=YAHOO_STOCK_URL, interactive=False, scale=1)\n",
        "            gr.Textbox(label=\"爬蟲模式\", value=\"專門針對 Yahoo 內頁擷取\", interactive=False, scale=1)\n",
        "\n",
        "        with gr.Tabs():\n",
        "\n",
        "            with gr.TabItem(\"🛠️ 技術日誌與輸出細節\"):\n",
        "                log_output_text = gr.Textbox(\n",
        "                    label=\"詳細流程日誌 (爬蟲、寫入、分析步驟)\",\n",
        "                    lines=30,\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "            with gr.TabItem(\"🕸️ 爬取文章列表\"):\n",
        "                site_list_output = gr.Radio(\n",
        "                    label=\"資料來源\",\n",
        "                    choices=[\"尚未執行\"],\n",
        "                    value=\"尚未執行\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                gr.Markdown(\"---\")\n",
        "\n",
        "                scraped_data_output = gr.Dataframe(\n",
        "                    label=\"爬取文章列表 (原始資料)\",\n",
        "                    headers=[\"日期\", \"作者\", \"標題\", \"連結\", \"內文\"],\n",
        "                    interactive=True,\n",
        "                    row_count=(15, 'dynamic')\n",
        "                )\n",
        "\n",
        "                link_display_output = gr.Markdown(\n",
        "          \t\t\tvalue=\"*原始文章資料已顯示於表格。*\"\n",
        "          \t\t)\n",
        "\n",
        "            with gr.TabItem(\"✅ 最終結果\"):\n",
        "                summary_output = gr.Markdown(label=\"🤖 Gemini 洞察摘要與結論\")\n",
        "\n",
        "                keyword_plot_output = gr.BarPlot(\n",
        "                  label=\"📈 Top N 熱詞視覺化圖表\",\n",
        "                  x=\"TF-IDF平均權重\",\n",
        "                  y=\"關鍵字\",\n",
        "                  tooltip=['關鍵字', 'TF-IDF平均權重'],\n",
        "                  color=\"TF-IDF平均權重\",\n",
        "                  vertical=False,\n",
        "                  height=400\n",
        "                )\n",
        "\n",
        "                keywords_output = gr.Dataframe(label=\"📈 Top N 熱詞統計結果 (Sklearn TF-IDF 總權重)\")\n",
        "\n",
        "\n",
        "        # === 綁定動作 ===\n",
        "        run_btn.click(\n",
        "          fn=run_full_automation_flow,\n",
        "          \tinputs=[top_n_input, articles_to_fetch_input],\n",
        "          \toutputs=[\n",
        "                log_output_text,\n",
        "                keywords_output,\n",
        "                summary_output,\n",
        "                keyword_plot_output,\n",
        "                site_list_output,\n",
        "                scraped_data_output\n",
        "            ]\n",
        "        )\n",
        "\n",
        "# 💡 關鍵修正：強制使用 share=True 來生成公開 URL\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "swFHjZVnO39T",
        "outputId": "994b6c94-35c9-4f1e-ad05-c88c693c788a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Google Sheets 授權成功。\n",
            "✅ Gemini API Key 配置成功。\n",
            "✅ 成功開啟 Sheet: HW4_文字資料小分析\n",
            "\n",
            "🚀 正在啟動 Gradio 介面...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e3f6210dade8186df3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e3f6210dade8186df3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e3f6210dade8186df3.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}