{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5vXkMmDazc4syNn/bX0UA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371120h/PL-Repo.peng/blob/main/HW4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 🔹 Yahoo 股市新聞分析 → TF-IDF → Gemini AI 洞察 (Sheet 強化版)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 運行環境設定（請在 Colab Cell 中執行）---\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "              gradio pandas beautifulsoup4 google-generativeai python-dateutil scikit-learn jieba\n",
        "\n",
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from requests.exceptions import RequestException, Timeout\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "import jieba  # 使用針對繁體中文優化的 jieba\n",
        "import jieba.analyse\n",
        "import jieba.posseg as pseg\n",
        "\n",
        "# Google Auth & Sheets\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "\n",
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import traceback\n",
        "import pytz\n",
        "\n",
        "# --- Google 認證與 Gemini 配置 ---\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    print(\"✅ Google Sheets 授權成功。\")\n",
        "\n",
        "    GEMINI_API_KEY = userdata.get(\"gemini\")\n",
        "    if not GEMINI_API_KEY:\n",
        "         raise ValueError(\"Colab Secret 'gemini' is empty or not found. Please set your Gemini API Key.\")\n",
        "\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    model = genai.GenerativeModel(\"gemini-2.5-flash\") # 使用更快速的模型\n",
        "    print(\"✅ Gemini API Key 配置成功。\")\n",
        "except Exception as e:\n",
        "    print(f\"🚨 授權或設定時發生錯誤：{e}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. 全域變數與 Sheet/DataFrame 設置\n",
        "# ==============================================================================\n",
        "# 請檢查您的 Sheet URL，確保正確\n",
        "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/107FcjXEnPn7vM10qFPj-wQFPeeNUOWTwKk5A-ejJqo4/edit?gid=412911623#gid=412911623\"\n",
        "TIMEZONE = \"Asia/Taipei\"\n",
        "\n",
        "# 確保這些欄位與 DF 輸出一致\n",
        "CLIPS_HEADER = [\"日期\", \"作者\", \"標題\", \"連結\", \"內文\"]\n",
        "STATS_HEADER = [\"關鍵字\", \"TF-IDF平均權重\"]\n",
        "SUMMARY_HEADER = [\"created_at\", \"keywords_used\", \"summary_report\"]\n",
        "\n",
        "# --- Google Sheet 初始化函式 (沿用您提供的邏輯) ---\n",
        "def get_or_create_worksheet(sheet, title):\n",
        "    try:\n",
        "        worksheet = sheet.worksheet(title)\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        worksheet = sheet.add_worksheet(title=title, rows=\"100\", cols=\"20\")\n",
        "    return worksheet\n",
        "\n",
        "def write_to_sheet(sheet, worksheet_name, df, log_output, header_list):\n",
        "    log_output.append(f\"--- 2. Google Sheet 寫入日誌 ---\")\n",
        "    try:\n",
        "        worksheet = get_or_create_worksheet(sheet, worksheet_name)\n",
        "        # 🚨 關鍵修正：確保寫入的欄位與 header_list 一致\n",
        "        if not df.empty:\n",
        "            df_to_write = df.reindex(columns=header_list, fill_value=\"\")\n",
        "\n",
        "            # 使用 update 寫入標題和數據\n",
        "            worksheet.clear()\n",
        "            worksheet.update(\n",
        "                [df_to_write.columns.values.tolist()] + df_to_write.astype(str).values.tolist(),\n",
        "                value_input_option=\"USER_ENTERED\"\n",
        "            )\n",
        "            log_output.append(f\"✅ 成功寫入 {worksheet_name} 工作表 ({len(df_to_write)} 筆資料)。\")\n",
        "        else:\n",
        "             worksheet.clear()\n",
        "             worksheet.update([header_list], value_input_option=\"USER_ENTERED\")\n",
        "             log_output.append(f\"✅ {worksheet_name} 工作表已清空 (無資料寫入)。\")\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"❌ 寫入 Sheet 失敗: {e}\")\n",
        "    return log_output\n",
        "\n",
        "# 開啟試算表並初始化工作表\n",
        "try:\n",
        "    gsheets = gc.open_by_url(SPREADSHEET_URL)\n",
        "    print(f\"✅ 成功開啟 Sheet: {gsheets.title}\")\n",
        "    # 這裡只確保工作表存在，實際寫入邏輯在 write_to_sheet 中\n",
        "    ws_clips = get_or_create_worksheet(gsheets, \"Yahoo文章列表\")\n",
        "    ws_stats = get_or_create_worksheet(gsheets, \"熱詞統計\")\n",
        "    ws_summary = get_or_create_worksheet(gsheets, \"AI摘要報告\")\n",
        "except Exception as e:\n",
        "     print(f\"❌ 無法初始化 Google Sheet: {e}\")\n",
        "     raise # 停止執行\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. 爬蟲、TF-IDF 統計與 Gemini 摘要\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 2.1 Yahoo 新聞爬蟲 (核心修正) ---\n",
        "YAHOO_STOCK_URL = \"https://tw.stock.yahoo.com/news\"\n",
        "\n",
        "def scrape_yahoo_stock_news(num_articles_to_fetch, log_output):\n",
        "    \"\"\"專門爬取 Yahoo 股市新聞指定文章數的文章列表與內文\"\"\"\n",
        "\n",
        "    # 使用通用的文章列表 selector\n",
        "    LIST_SELECTOR = \"a[href*='tw.stock.yahoo.com/news/']\"\n",
        "\n",
        "    session = requests.Session()\n",
        "    all_data_list = []\n",
        "    log_output.append(f\"--- 1. 爬蟲日誌 ---\")\n",
        "    log_output.append(f\"目標網站: Yahoo 股市新聞 | 爬取文章數: {num_articles_to_fetch}\")\n",
        "\n",
        "    # 使用標準 Headers 模擬瀏覽器\n",
        "    enhanced_headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "        \"Accept-Language\": \"zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # 1. 爬取列表頁\n",
        "        r = session.get(YAHOO_STOCK_URL, timeout=15, headers=enhanced_headers)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        article_links = soup.select(LIST_SELECTOR)[:num_articles_to_fetch]\n",
        "\n",
        "        log_output.append(f\"列表頁找到 {len(article_links)} 篇文章連結。\")\n",
        "\n",
        "        # 2. 爬取內頁\n",
        "        for i, a_tag in enumerate(article_links):\n",
        "            link = a_tag.get(\"href\")\n",
        "            if not link or \"javascript:void(0)\" in link: continue\n",
        "\n",
        "            # 處理相對路徑\n",
        "            if not link.startswith(\"http\"):\n",
        "                from urllib.parse import urljoin\n",
        "                link = urljoin(YAHOO_STOCK_URL, link)\n",
        "\n",
        "            # 爬取內頁\n",
        "            try:\n",
        "                sub_resp = session.get(link, timeout=10, headers=enhanced_headers)\n",
        "                sub_resp.raise_for_status()\n",
        "                sub_soup = BeautifulSoup(sub_resp.text, \"html.parser\")\n",
        "\n",
        "                # 抓取標題 (h1)\n",
        "                title = sub_soup.select_one(\"h1\").get_text(strip=True) if sub_soup.select_one(\"h1\") else \"無標題\"\n",
        "\n",
        "                # 抓取內容 (p 標籤內文)\n",
        "                content_nodes = sub_soup.select(\"p\")\n",
        "                content = \" \".join([p.get_text(strip=True) for p in content_nodes if len(p.get_text(strip=True)) > 20])\n",
        "\n",
        "                # 抓取作者/日期 (通常在特定的 span/div 內，這裡使用簡化方式)\n",
        "                date_node = sub_soup.select_one(\"time\")\n",
        "                date_str = date_node.get(\"datetime\") if date_node and date_node.get(\"datetime\") else dt.now(gettz(TIMEZONE)).strftime(\"%m/%d\")\n",
        "                author = sub_soup.select_one(\"span.author-name\")\n",
        "                author_str = author.get_text(strip=True) if author else \"Yahoo 股市\"\n",
        "\n",
        "                all_data_list.append({\n",
        "                    \"日期\": date_str,\n",
        "                    \"作者\": author_str,\n",
        "                    \"標題\": title,\n",
        "                    \"連結\": link,\n",
        "                    \"內文\": content\n",
        "                })\n",
        "                log_output.append(f\"   -> 成功擷取 #{i+1}: {title[:20]}...\")\n",
        "            except Exception as e:\n",
        "                log_output.append(f\"   ⚠️ 爬取或解析內頁失敗 ({link}): {e}\")\n",
        "                continue\n",
        "\n",
        "            time.sleep(0.1)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        log_output.append(f\"❌ 爬蟲起始請求失敗：{e}\")\n",
        "\n",
        "    df = pd.DataFrame(all_data_list)\n",
        "    log_output.append(f\"✅ 爬蟲結束。共抓取 {len(df)} 篇文章。\")\n",
        "    return df, log_output\n",
        "\n",
        "# --- 2.2 TF-IDF 關鍵字分析 (中文金融優化) ---\n",
        "\n",
        "STOPWORDS = set([\n",
        "    '的', '了', '是', '在', '我', '你', '他', '她', '之', '一個', '和', '與', '或', '也', '都', '將',\n",
        "    '被', '由', '所', '於', '於此', '這', '那', '而', '但', '並', '則', '要', '應', '進行', '如果',\n",
        "    # 數字與單位\n",
        "    '元', '萬元', '億元', '萬', '億', '千', '百', '個', '日', '月', '年', '季', '週', '天', '點', '度',\n",
        "    # 常用詞\n",
        "    '公司', '企業', '市場', '指出', '表示', '報導', '分析', '認為', '提供', '資訊', '網站', '股價', '股市',\n",
        "    '投資', '交易', '客戶', '業務', '產品', '服務', '資料', '已經', '不過', '此外', '目前', '未來', '預計',\n",
        "    # 介系詞與連詞\n",
        "    '對於', '關於', '由於', '因為', '隨著', '除了', '包括', '例如', '如果說', '甚至', '還是', '還是說'\n",
        "])\n",
        "\n",
        "def chinese_tokenizer(text):\n",
        "    \"\"\"分詞並過濾停用詞和單字\"\"\"\n",
        "    # 移除標點符號和數字\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', ' ', text).strip()\n",
        "    cleaned_text = re.sub(r'\\d+', ' ', cleaned_text)\n",
        "\n",
        "    # 使用精確模式分詞\n",
        "    words = jieba.lcut(cleaned_text, cut_all=False)\n",
        "\n",
        "    # 過濾停用詞和單字\n",
        "    filtered_words = [\n",
        "        word.strip()\n",
        "        for word in words\n",
        "        if word.strip() and len(word.strip()) > 1 and word.strip().lower() not in STOPWORDS\n",
        "    ]\n",
        "    return filtered_words\n",
        "\n",
        "\n",
        "def get_tfidf_keywords(df, top_n, log_output):\n",
        "    \"\"\"使用 sklearn.TfidfVectorizer 進行 TF-IDF 分析\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 3. TF-IDF 分析日誌 (Sklearn) ---\")\n",
        "    log_output.append(f\"目標關鍵字數量: Top {top_n}\")\n",
        "    log_output.append(f\"停用詞數量: {len(STOPWORDS)}\")\n",
        "\n",
        "    if '內文' not in df.columns or df['內文'].dropna().empty:\n",
        "        log_output.append(\"❌ 錯誤: 資料集中缺少 '內文' 欄位或內文為空。\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "    document_list = []\n",
        "\n",
        "    # 1. 文本預處理與分詞\n",
        "    for content in df['內文'].dropna():\n",
        "        filtered_words = chinese_tokenizer(content)\n",
        "        if filtered_words:\n",
        "            document_list.append(\" \".join(filtered_words))\n",
        "\n",
        "    log_output.append(f\"已將 {len(df)} 篇文章分詞並過濾，產生 {len(document_list)} 篇有效文檔。\")\n",
        "\n",
        "    if not document_list:\n",
        "        log_output.append(\"⚠️ 沒有可分析的文檔 (可能都被過濾了)。\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "    # 2. TF-IDF 計算\n",
        "    try:\n",
        "        # 使用自定義分詞器，並設定 n-gram (1-gram 和 2-gram)\n",
        "        vectorizer = TfidfVectorizer(tokenizer=chinese_tokenizer, ngram_range=(1, 2))\n",
        "        tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        log_output.append(f\"✅ TF-IDF 矩陣建立成功。詞彙總數 (Features): {len(feature_names)}\")\n",
        "\n",
        "        # 3. 計算平均權重 (找出整體重要性)\n",
        "        # 🚨 關鍵修正: 使用 sum(axis=0) 計算總權重而非平均，在不同文件數量下更穩定\n",
        "        sum_tfidf_scores = tfidf_matrix.sum(axis=0).tolist()[0]\n",
        "\n",
        "        # 4. 建立 DataFrame 排序並選出 Top N\n",
        "        keywords_with_scores = list(zip(feature_names, sum_tfidf_scores))\n",
        "\n",
        "        # 排序\n",
        "        sorted_keywords = sorted(keywords_with_scores, key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        top_keywords_df = pd.DataFrame(\n",
        "            [(k, round(s, 4)) for k, s in sorted_keywords[:top_n]],\n",
        "            columns=['關鍵字', 'TF-IDF平均權重']\n",
        "        )\n",
        "\n",
        "        log_output.append(f\"✅ 成功提取 Top {len(top_keywords_df)} 個關鍵字。\")\n",
        "        log_output.append(f\"Top 5 關鍵字範例: {', '.join(top_keywords_df['關鍵字'].head(5).tolist())}\")\n",
        "\n",
        "        return top_keywords_df, log_output\n",
        "\n",
        "    except ValueError as e:\n",
        "        log_output.append(f\"❌ TF-IDF 分析失敗 (ValueError): {e}\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"❌ TF-IDF 分析發生未預期錯誤: {e}\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "\n",
        "# --- 2.3 Gemini API 生成摘要 (保留高超時設定) ---\n",
        "def get_gemini_summary(keywords_df, log_output):\n",
        "    \"\"\"使用 Gemini API 根據關鍵字生成摘要，並將結果寫入 Sheet\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 4. Gemini 摘要日誌 ---\")\n",
        "\n",
        "    if keywords_df.empty:\n",
        "        log_output.append(\"⚠️ 缺少關鍵字，無法生成摘要。\")\n",
        "        return \"⚠️ 沒有關鍵字，無法生成摘要。\", log_output\n",
        "\n",
        "    keywords_list = keywords_df['關鍵字'].tolist()\n",
        "    prompt = f\"\"\"\n",
        "    您是一位專業的股市數據分析師。\n",
        "\n",
        "    任務：\n",
        "    請根據 Yahoo 股市新聞的 {len(keywords_list)} 個熱門關鍵字，生成一份專業的股市分析報告。\n",
        "\n",
        "    熱門關鍵字 (依 TF-IDF 總權重排序)：\n",
        "    {', '.join(keywords_list)}\n",
        "\n",
        "    輸出格式要求 (請嚴格遵守)：\n",
        "    1.  **五句洞察摘要**：條列式，每句都是精闢的股市觀察。\n",
        "    2.  **一段 120 字結論**：總結目前的股市趨勢或投資機會。\n",
        "\n",
        "    請使用繁體中文回答。\n",
        "    \"\"\"\n",
        "\n",
        "    summary_text = \"\"\n",
        "    try:\n",
        "        log_output.append(f\"模型請求參數: gemini-2.5-flash, 關鍵字數量: {len(keywords_list)}\")\n",
        "        # 🚨 關鍵修正：保持 120 秒超時，應對連線不穩定的問題\n",
        "        response = model.generate_content(prompt, request_options={\"timeout\": 120})\n",
        "\n",
        "        summary_text = response.text.replace(\"#\", \"\").replace(\"*\", \"\")\n",
        "        log_output.append(\"✅ 摘要生成成功。\")\n",
        "        msg_status = \"✅ Gemini 摘要生成完成。\"\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Gemini API 呼叫失敗：{e}\"\n",
        "        log_output.append(error_msg)\n",
        "        summary_text = error_msg\n",
        "        msg_status = f\"⚠️ Gemini 請求失敗，詳情請看日誌。\"\n",
        "\n",
        "    # --- 寫入 AI 摘要至 Sheet ---\n",
        "    try:\n",
        "        # 將最新的摘要寫入 'AI摘要報告' 工作表\n",
        "        new_row_df = pd.DataFrame([{\n",
        "            \"created_at\": dt.now(gettz(TIMEZONE)).isoformat(),\n",
        "            \"keywords_used\": \", \".join(keywords_list[:10]), # 只寫入前 10 個關鍵字\n",
        "            \"summary_report\": summary_text\n",
        "        }], columns=SUMMARY_HEADER)\n",
        "\n",
        "        # 由於您原程式中沒有 read_df for summary，這裡使用 gspread 的 append 寫入 (放在最前面需要更多邏輯，先用寫入單行替代)\n",
        "        # 確保在 Sheet 中，此工作表 'AI摘要報告' 已經有 SUMMARY_HEADER\n",
        "\n",
        "        # 🚨 修正：使用 df 的前幾行模擬寫入邏輯\n",
        "        # 讀取現有數據，將新數據插入最前面，然後寫回 (確保最新的在上方)\n",
        "        df_existing = ws_summary.get_all_values()\n",
        "        df_existing_data = [row for row in df_existing if row != SUMMARY_HEADER]\n",
        "\n",
        "        # 準備新資料列 (必須是 list of lists)\n",
        "        new_data_row = new_row_df.iloc[0].values.tolist()\n",
        "\n",
        "        # 清空工作表並寫入新的標題和數據\n",
        "        ws_summary.clear()\n",
        "        ws_summary.update([SUMMARY_HEADER] + [new_data_row] + df_existing_data, value_input_option=\"USER_ENTERED\")\n",
        "\n",
        "        log_output.append(\"✅ 摘要和關鍵詞已成功寫入 'AI摘要報告' 工作表。\")\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"❌ 寫入 AI 摘要至 Sheet 失敗: {e}\")\n",
        "\n",
        "    return summary_text, log_output\n",
        "\n",
        "# ================================\n",
        "# 3. Gradio 整合函式\n",
        "# ================================\n",
        "def run_full_automation_flow(top_n_str, articles_to_fetch_str):\n",
        "    \"\"\"Gradio 點擊後執行的完整流程\"\"\"\n",
        "\n",
        "    empty_df = pd.DataFrame(columns=STATS_HEADER)\n",
        "    empty_scraped_df = pd.DataFrame(columns=CLIPS_HEADER)\n",
        "    empty_str = \"\"\n",
        "    log_output = []\n",
        "\n",
        "    SITE_NAME = \"Yahoo 股市新聞\"\n",
        "    site_list = [SITE_NAME]\n",
        "\n",
        "    # 清空上次結果\n",
        "    yield \"日誌將顯示於此...\", empty_df, empty_str, None, gr.Radio(choices=[\"尚未執行\"], value=\"尚未執行\"), empty_scraped_df\n",
        "\n",
        "    # --- 參數驗證 ---\n",
        "    try:\n",
        "        top_n = int(top_n_str)\n",
        "        articles_to_fetch = int(articles_to_fetch_str)\n",
        "        if top_n <= 0 or articles_to_fetch <= 0:\n",
        "            log_output.append(\"❌ Top N 或爬取文章數必須是大於 0 的數字。\")\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), empty_scraped_df\n",
        "            return\n",
        "    except ValueError:\n",
        "        log_output.append(\"❌ 請輸入有效的數字。\")\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), empty_scraped_df\n",
        "        return\n",
        "\n",
        "    # --- 自動化流程 ---\n",
        "    log_output.append(\"===================================================\")\n",
        "    current_time_str = dt.now(gettz(TIMEZONE)).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    log_output.append(f\"🚀 自動化流程啟動 ({current_time_str})\")\n",
        "    log_output.append(\"===================================================\")\n",
        "\n",
        "    try:\n",
        "        # --- 步驟 1: 爬蟲 (Yahoo News) ---\n",
        "        log_output.append(f\"1/4: 🏃‍♂️ 開始爬取 {SITE_NAME} 文章，請稍等...\")\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), empty_scraped_df\n",
        "\n",
        "        scraped_df, log_output = scrape_yahoo_stock_news(articles_to_fetch, log_output)\n",
        "        display_df = scraped_df[[\"日期\", \"作者\", \"標題\", \"連結\", \"內文\"]] # 確保顯示正確欄位\n",
        "\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "\n",
        "        if scraped_df.empty:\n",
        "            log_output.append(\"❌ 爬蟲失敗，未抓取到任何資料。流程終止。\")\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), empty_scraped_df\n",
        "            return\n",
        "\n",
        "        # --- 步驟 2: 寫入 Sheet (文章列表) ---\n",
        "        log_output = write_to_sheet(gsheets, \"Yahoo文章列表\", scraped_df, log_output, CLIPS_HEADER)\n",
        "\n",
        "        # --- 步驟 3: TF-IDF 分析 ---\n",
        "        log_output.append(\"2/4: 📊 正在進行 Sklearn TF-IDF 關鍵字分析...\")\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "\n",
        "        keywords_df, log_output = get_tfidf_keywords(scraped_df, top_n, log_output)\n",
        "\n",
        "        plot_df = None\n",
        "        if not keywords_df.empty:\n",
        "          # 為了讓 BarPlot 頂部顯示權重最高的，我們需將 df 升冪排序\n",
        "          plot_df = keywords_df.sort_values(\"TF-IDF平均權重\", ascending=True)\n",
        "\n",
        "        yield \"\\n\".join(log_output), keywords_df, empty_str, plot_df, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "\n",
        "        if keywords_df.empty:\n",
        "            log_output.append(\"⚠️ 分析完成，但未提取到關鍵字。流程終止。\")\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "            return\n",
        "\n",
        "        # --- 步驟 4: 寫入 Sheet (熱詞統計) ---\n",
        "        log_output.append(\"3/4: 📈 正在將 Top 熱詞回寫至 Sheet (熱詞統計)...\")\n",
        "        log_output = write_to_sheet(gsheets, \"熱詞統計\", keywords_df, log_output, STATS_HEADER)\n",
        "\n",
        "        # --- 步驟 5: Gemini 摘要與寫入 Sheet (AI摘要報告) ---\n",
        "        log_output.append(\"4/4: 🧠 正在呼叫 Gemini API 生成摘要 (已設定 120s 超時)...\")\n",
        "        yield \"\\n\".join(log_output), keywords_df, empty_str, plot_df, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "\n",
        "        summary, log_output = get_gemini_summary(keywords_df, log_output)\n",
        "\n",
        "        # 最終回傳\n",
        "        final_log = \"\\n\".join(log_output)\n",
        "        yield final_log, keywords_df, summary, plot_df, gr.Radio(choices=site_list, value=SITE_NAME), display_df\n",
        "\n",
        "        log_output.append(\"===================================================\")\n",
        "        log_output.append(\"✅ 全部流程完成！請切換到「最終結果」標籤頁查看。\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ 流程發生未預期錯誤：{e}\\n{traceback.format_exc()}\"\n",
        "        log_output.append(error_msg)\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), empty_scraped_df\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4. 啟動 Gradio 介面 (已修改為 Yahoo)\n",
        "# ================================\n",
        "print(\"\\n🚀 正在啟動 Gradio 介面...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"orange\"), title=\"Yahoo 股市新聞分析與 AI 摘要（Sheet 強化版）\") as demo:\n",
        "    gr.Markdown(\n",
        "\n",
        "        \"\"\"\n",
        "        # 📈 Yahoo 股市新聞分析 → TF-IDF 關鍵詞 → AI 洞察摘要\n",
        "        此工具會自動執行：**Yahoo 爬蟲 → 寫入 Sheet (文章列表) → TF-IDF 統計 → 寫入 Sheet (熱詞統計) → Gemini 生成摘要 → 寫入 Sheet (AI摘要報告)**。\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"🚀 自動化流程執行\"):\n",
        "        with gr.Row():\n",
        "            # 🚨 修正：參數名稱從 pages_to_fetch 改為 articles_to_fetch\n",
        "            articles_to_fetch_input = gr.Textbox(label=\"要爬取的文章數量 (Limit)\", value=\"10\", scale=1)\n",
        "            top_n_input = gr.Textbox(label=\"要統計的 Top N 熱詞數量\", value=\"20\", scale=1)\n",
        "            run_btn = gr.Button(\"🚀 一鍵啟動 Yahoo 股市新聞分析\", variant=\"primary\", scale=2)\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        # 顯示 Yahoo 爬蟲的參數資訊\n",
        "        with gr.Row():\n",
        "            gr.Textbox(label=\"目標網站\", value=YAHOO_STOCK_URL, interactive=False, scale=1)\n",
        "            gr.Textbox(label=\"爬蟲模式\", value=\"專門針對 Yahoo 內頁擷取\", interactive=False, scale=1)\n",
        "\n",
        "        # 使用 Tab 來區分最終結果和日誌\n",
        "        with gr.Tabs():\n",
        "\n",
        "            with gr.TabItem(\"🛠️ 技術日誌與輸出細節\"):\n",
        "                log_output_text = gr.Textbox(\n",
        "                    label=\"詳細流程日誌 (爬蟲、寫入、分析步驟)\",\n",
        "                    lines=30,\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "            with gr.TabItem(\"🕸️ 爬取文章列表\"):\n",
        "                site_list_output = gr.Radio(\n",
        "                    label=\"資料來源\",\n",
        "                    choices=[\"尚未執行\"],\n",
        "                    value=\"尚未執行\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                gr.Markdown(\"---\")\n",
        "\n",
        "                # 由於爬蟲邏輯已改，這裡只顯示 DataFrame，點擊連結功能暫不實作\n",
        "                scraped_data_output = gr.Dataframe(\n",
        "                    label=\"爬取文章列表 (原始資料)\",\n",
        "                    headers=[\"日期\", \"作者\", \"標題\", \"連結\", \"內文\"],\n",
        "                    interactive=True,\n",
        "                    row_count=(15, 'dynamic')\n",
        "                )\n",
        "\n",
        "                # 重新綁定 Dataframe 點擊事件，確保即使連結功能未實作，也不會報錯\n",
        "                link_display_output = gr.Markdown(\n",
        "          \t\t\tvalue=\"*原始文章資料已顯示於表格。*\"\n",
        "          \t\t)\n",
        "                # 移除 show_selected_link 點擊事件，避免 Gradio 錯誤\n",
        "\n",
        "            with gr.TabItem(\"✅ 最終結果\"):\n",
        "                summary_output = gr.Markdown(label=\"🤖 Gemini 洞察摘要與結論\")\n",
        "\n",
        "                keyword_plot_output = gr.BarPlot(\n",
        "                  label=\"📈 Top N 熱詞視覺化圖表\",\n",
        "                  x=\"TF-IDF平均權重\",\n",
        "                  y=\"關鍵字\",\n",
        "                  tooltip=['關鍵字', 'TF-IDF平均權重'],\n",
        "                  color=\"TF-IDF平均權重\",\n",
        "                  vertical=False,\n",
        "                  height=400\n",
        "                )\n",
        "\n",
        "                keywords_output = gr.Dataframe(label=\"📈 Top N 熱詞統計結果 (Sklearn TF-IDF 總權重)\")\n",
        "\n",
        "\n",
        "        # === 綁定動作 ===\n",
        "        run_btn.click(\n",
        "          fn=run_full_automation_flow,\n",
        "          \tinputs=[top_n_input, articles_to_fetch_input],\n",
        "          \toutputs=[\n",
        "                log_output_text,\n",
        "                keywords_output,\n",
        "                summary_output,\n",
        "                keyword_plot_output,\n",
        "                site_list_output,\n",
        "                scraped_data_output\n",
        "            ]\n",
        "        )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "PXpEZsmjx4Yw",
        "outputId": "cb795c5f-c22f-4f39-878a-aea0608bc001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Google Sheets 授權成功。\n",
            "✅ Gemini API Key 配置成功。\n",
            "✅ 成功開啟 Sheet: HW4_文字資料小分析\n",
            "\n",
            "🚀 正在啟動 Gradio 介面...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://228a41780666910844.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://228a41780666910844.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}