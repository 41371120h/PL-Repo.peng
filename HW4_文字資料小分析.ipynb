{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy8GI/4qveKs7qn2Eq7Io+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371120h/PL-Repo.peng/blob/main/HW4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#è©¦ç®—è¡¨é€£çµï¼šhttps://docs.google.com/spreadsheets/d/107FcjXEnPn7vM10qFPj-wQFPeeNUOWTwKk5A-ejJqo4/edit?gid=616863925#gid=616863925"
      ],
      "metadata": {
        "id": "Msjc1h4KQj-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ğŸ”¹ Yahoo è‚¡å¸‚æ–°èåˆ†æ â†’ TF-IDF â†’ Gemini AI æ´å¯Ÿ (æœ€çµ‚ç©©å®šç‰ˆ V4)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- é‹è¡Œç’°å¢ƒè¨­å®šï¼ˆè«‹åœ¨ Colab Cell ä¸­åŸ·è¡Œï¼‰---\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "              gradio pandas beautifulsoup4 google-generativeai python-dateutil scikit-learn jieba\n",
        "\n",
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from requests.exceptions import RequestException, Timeout\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "import jieba.posseg as pseg\n",
        "\n",
        "# Google Auth & Sheets\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "\n",
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import traceback\n",
        "import pytz\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. é—œéµåˆå§‹åŒ–å€å¡Š (ç¢ºä¿æ­¤å€å¡ŠæˆåŠŸåŸ·è¡Œ)\n",
        "# ==============================================================================\n",
        "gc = None\n",
        "gsheets = None\n",
        "gemini_model = None\n",
        "\n",
        "# è«‹æª¢æŸ¥æ‚¨çš„ Sheet URLï¼Œç¢ºä¿æ­£ç¢º\n",
        "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/107FcjXEnPn7vM10qFPj-wQFPeeNUOWTwKk5A-ejJqo4/edit?gid=616863925#gid=616863925\"\n",
        "TIMEZONE = \"Asia/Taipei\"\n",
        "\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    print(\"âœ… Google Sheets æˆæ¬ŠæˆåŠŸã€‚\")\n",
        "\n",
        "    GEMINI_API_KEY = userdata.get(\"gemini\")\n",
        "    if not GEMINI_API_KEY:\n",
        "         raise ValueError(\"Colab Secret 'gemini' is empty or not found. Please set your Gemini API Key.\")\n",
        "\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    gemini_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "    print(\"âœ… Gemini API Key é…ç½®æˆåŠŸã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸš¨ æˆæ¬Šæˆ–è¨­å®šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
        "    gsheets = None\n",
        "\n",
        "# ç¢ºä¿é€™äº›æ¬„ä½èˆ‡ DF è¼¸å‡ºä¸€è‡´\n",
        "CLIPS_HEADER = [\"æ—¥æœŸ\", \"ä½œè€…\", \"æ¨™é¡Œ\", \"é€£çµ\", \"å…§æ–‡\"]\n",
        "STATS_HEADER = [\"é—œéµå­—\", \"TF-IDFå¹³å‡æ¬Šé‡\"]\n",
        "SUMMARY_HEADER = [\"created_at\", \"keywords_used\", \"summary_report\"]\n",
        "\n",
        "# ... (çœç•¥ 1. Google Sheet è¨­ç½® å’Œ 2.1 Yahoo æ–°èçˆ¬èŸ²ï¼Œèˆ‡ V3 ç‰ˆç›¸åŒ) ...\n",
        "# ==============================================================================\n",
        "# 1. Google Sheet è¨­ç½®\n",
        "# ==============================================================================\n",
        "def get_or_create_worksheet(sheet, title):\n",
        "    try:\n",
        "        worksheet = sheet.worksheet(title)\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        worksheet = sheet.add_worksheet(title=title, rows=\"100\", cols=\"20\")\n",
        "    return worksheet\n",
        "\n",
        "def write_to_sheet(sheet, worksheet_name, df, log_output, header_list):\n",
        "    log_output.append(f\"--- 2. Google Sheet å¯«å…¥æ—¥èªŒ ---\")\n",
        "\n",
        "    if sheet is None:\n",
        "        log_output.append(\"âŒ Google Sheet é€£ç·šå¤±æ•—ï¼Œè·³éå¯«å…¥ã€‚\")\n",
        "        return log_output\n",
        "\n",
        "    try:\n",
        "        worksheet = get_or_create_worksheet(sheet, worksheet_name)\n",
        "        if not df.empty:\n",
        "            df_to_write = df.reindex(columns=header_list, fill_value=\"\")\n",
        "\n",
        "            worksheet.clear()\n",
        "            worksheet.update(\n",
        "                [df_to_write.columns.values.tolist()] + df_to_write.astype(str).values.tolist(),\n",
        "                value_input_option=\"USER_ENTERED\"\n",
        "            )\n",
        "            log_output.append(f\"âœ… æˆåŠŸå¯«å…¥ {worksheet_name} å·¥ä½œè¡¨ ({len(df_to_write)} ç­†è³‡æ–™)ã€‚\")\n",
        "        else:\n",
        "             worksheet.clear()\n",
        "             worksheet.update([header_list], value_input_option=\"USER_ENTERED\")\n",
        "             log_output.append(f\"âœ… {worksheet_name} å·¥ä½œè¡¨å·²æ¸…ç©º (ç„¡è³‡æ–™å¯«å…¥)ã€‚\")\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"âŒ å¯«å…¥ Sheet å¤±æ•—: {e}\")\n",
        "    return log_output\n",
        "\n",
        "# é–‹å•Ÿè©¦ç®—è¡¨ä¸¦åˆå§‹åŒ–å·¥ä½œè¡¨\n",
        "ws_summary = None\n",
        "if gsheets is None and gc:\n",
        "    try:\n",
        "        gsheets = gc.open_by_url(SPREADSHEET_URL)\n",
        "        print(f\"âœ… æˆåŠŸé–‹å•Ÿ Sheet: {gsheets.title}\")\n",
        "        ws_clips = get_or_create_worksheet(gsheets, \"Yahooæ–‡ç« åˆ—è¡¨\")\n",
        "        ws_stats = get_or_create_worksheet(gsheets, \"ç†±è©çµ±è¨ˆ\")\n",
        "        ws_summary = get_or_create_worksheet(gsheets, \"AIæ‘˜è¦å ±å‘Š\")\n",
        "    except Exception as e:\n",
        "         print(f\"âŒ ç„¡æ³•åˆå§‹åŒ– Google Sheet: {e}\")\n",
        "         gsheets = None\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. çˆ¬èŸ²ã€TF-IDF çµ±è¨ˆèˆ‡ Gemini æ‘˜è¦\n",
        "# ==============================================================================\n",
        "YAHOO_STOCK_URL = \"https://tw.stock.yahoo.com/news\"\n",
        "\n",
        "def scrape_yahoo_stock_news(num_articles_to_fetch, log_output):\n",
        "    \"\"\"å°ˆé–€çˆ¬å– Yahoo è‚¡å¸‚æ–°èæŒ‡å®šæ–‡ç« æ•¸çš„æ–‡ç« åˆ—è¡¨èˆ‡å…§æ–‡\"\"\"\n",
        "\n",
        "    LIST_SELECTOR = \"a[href*='tw.stock.yahoo.com/news/']\"\n",
        "    session = requests.Session()\n",
        "    all_data_list = []\n",
        "    log_output.append(f\"--- 1. çˆ¬èŸ²æ—¥èªŒ ---\")\n",
        "    log_output.append(f\"ç›®æ¨™ç¶²ç«™: Yahoo è‚¡å¸‚æ–°è | çˆ¬å–æ–‡ç« æ•¸: {num_articles_to_fetch}\")\n",
        "\n",
        "    enhanced_headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "        \"Accept-Language\": \"zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = session.get(YAHOO_STOCK_URL, timeout=15, headers=enhanced_headers)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        article_links = soup.select(LIST_SELECTOR)[:num_articles_to_fetch]\n",
        "\n",
        "        log_output.append(f\"åˆ—è¡¨é æ‰¾åˆ° {len(article_links)} ç¯‡æ–‡ç« é€£çµã€‚\")\n",
        "\n",
        "        for i, a_tag in enumerate(article_links):\n",
        "            link = a_tag.get(\"href\")\n",
        "            if not link or \"javascript:void(0)\" in link: continue\n",
        "\n",
        "            if not link.startswith(\"http\"):\n",
        "                from urllib.parse import urljoin\n",
        "                link = urljoin(YAHOO_STOCK_URL, link)\n",
        "\n",
        "            try:\n",
        "                sub_resp = session.get(link, timeout=10, headers=enhanced_headers)\n",
        "                sub_resp.raise_for_status()\n",
        "                sub_soup = BeautifulSoup(sub_resp.text, \"html.parser\")\n",
        "\n",
        "                title = sub_soup.select_one(\"h1\").get_text(strip=True) if sub_soup.select_one(\"h1\") else \"ç„¡æ¨™é¡Œ\"\n",
        "\n",
        "                content_nodes = sub_soup.select(\"p\")\n",
        "                content = \" \".join([p.get_text(strip=True) for p in content_nodes if len(p.get_text(strip=True)) > 20])\n",
        "\n",
        "                date_node = sub_soup.select_one(\"time\")\n",
        "                date_str = date_node.get(\"datetime\") if date_node and date_node.get(\"datetime\") else dt.now(gettz(TIMEZONE)).strftime(\"%m/%d\")\n",
        "                author = sub_soup.select_one(\"span.author-name\")\n",
        "                author_str = author.get_text(strip=True) if author else \"Yahoo è‚¡å¸‚\"\n",
        "\n",
        "                all_data_list.append({\n",
        "                    \"æ—¥æœŸ\": date_str,\n",
        "                    \"ä½œè€…\": author_str,\n",
        "                    \"æ¨™é¡Œ\": title,\n",
        "                    \"é€£çµ\": link,\n",
        "                    \"å…§æ–‡\": content\n",
        "                })\n",
        "                log_output.append(f\"   -> æˆåŠŸæ“·å– #{i+1}: {title[:20]}...\")\n",
        "            except Exception as e:\n",
        "                log_output.append(f\"   âš ï¸ çˆ¬å–æˆ–è§£æå…§é å¤±æ•— ({link}): {e}\")\n",
        "                continue\n",
        "\n",
        "            time.sleep(0.1)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        log_output.append(f\"âŒ çˆ¬èŸ²èµ·å§‹è«‹æ±‚å¤±æ•—ï¼š{e}\")\n",
        "\n",
        "    df = pd.DataFrame(all_data_list)\n",
        "    log_output.append(f\"âœ… çˆ¬èŸ²çµæŸã€‚å…±æŠ“å– {len(df)} ç¯‡æ–‡ç« ã€‚\")\n",
        "    return df, log_output\n",
        "\n",
        "# --- 2.2 TF-IDF é—œéµå­—åˆ†æ ---\n",
        "STOPWORDS = set([\n",
        "    'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'ä¹‹', 'ä¸€å€‹', 'å’Œ', 'èˆ‡', 'æˆ–', 'ä¹Ÿ', 'éƒ½', 'å°‡',\n",
        "    'è¢«', 'ç”±', 'æ‰€', 'æ–¼', 'æ–¼æ­¤', 'é€™', 'é‚£', 'è€Œ', 'ä½†', 'ä¸¦', 'å‰‡', 'è¦', 'æ‡‰', 'é€²è¡Œ', 'å¦‚æœ',\n",
        "    'å…ƒ', 'è¬å…ƒ', 'å„„å…ƒ', 'è¬', 'å„„', 'åƒ', 'ç™¾', 'å€‹', 'æ—¥', 'æœˆ', 'å¹´', 'å­£', 'é€±', 'å¤©', 'é»', 'åº¦',\n",
        "    'å…¬å¸', 'ä¼æ¥­', 'å¸‚å ´', 'æŒ‡å‡º', 'è¡¨ç¤º', 'å ±å°', 'åˆ†æ', 'èªç‚º', 'æä¾›', 'è³‡è¨Š', 'ç¶²ç«™', 'è‚¡åƒ¹', 'è‚¡å¸‚',\n",
        "    'æŠ•è³‡', 'äº¤æ˜“', 'å®¢æˆ¶', 'æ¥­å‹™', 'ç”¢å“', 'æœå‹™', 'è³‡æ–™', 'å·²ç¶“', 'ä¸é', 'æ­¤å¤–', 'ç›®å‰', 'æœªä¾†', 'é è¨ˆ',\n",
        "    'å°æ–¼', 'é—œæ–¼', 'ç”±æ–¼', 'å› ç‚º', 'éš¨è‘—', 'é™¤äº†', 'åŒ…æ‹¬', 'ä¾‹å¦‚', 'å¦‚æœèªª', 'ç”šè‡³', 'é‚„æ˜¯', 'é‚„æ˜¯èªª'\n",
        "])\n",
        "\n",
        "def chinese_tokenizer(text):\n",
        "    \"\"\"åˆ†è©ä¸¦éæ¿¾åœç”¨è©å’Œå–®å­—\"\"\"\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', ' ', text).strip()\n",
        "    cleaned_text = re.sub(r'\\d+', ' ', cleaned_text)\n",
        "\n",
        "    words = jieba.lcut(cleaned_text, cut_all=False)\n",
        "\n",
        "    filtered_words = [\n",
        "        word.strip()\n",
        "        for word in words\n",
        "        if word.strip() and len(word.strip()) > 1 and word.strip().lower() not in STOPWORDS\n",
        "    ]\n",
        "    return filtered_words\n",
        "\n",
        "\n",
        "def get_tfidf_keywords(df, top_n, log_output):\n",
        "    \"\"\"ä½¿ç”¨ sklearn.TfidfVectorizer é€²è¡Œ TF-IDF åˆ†æ\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 3. TF-IDF åˆ†ææ—¥èªŒ (Sklearn) ---\")\n",
        "\n",
        "    if 'å…§æ–‡' not in df.columns or df['å…§æ–‡'].dropna().empty:\n",
        "        log_output.append(\"âŒ éŒ¯èª¤: è³‡æ–™é›†ä¸­ç¼ºå°‘ 'å…§æ–‡' æ¬„ä½æˆ–å…§æ–‡ç‚ºç©ºã€‚\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "    document_list = []\n",
        "\n",
        "    for content in df['å…§æ–‡'].dropna():\n",
        "        filtered_words = chinese_tokenizer(content)\n",
        "        if filtered_words:\n",
        "            document_list.append(\" \".join(filtered_words))\n",
        "\n",
        "    if not document_list:\n",
        "        log_output.append(\"âš ï¸ æ²’æœ‰å¯åˆ†æçš„æ–‡æª” (å¯èƒ½éƒ½è¢«éæ¿¾äº†)ã€‚\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(tokenizer=chinese_tokenizer, ngram_range=(1, 2))\n",
        "        tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        sum_tfidf_scores = tfidf_matrix.sum(axis=0).tolist()[0]\n",
        "\n",
        "        keywords_with_scores = list(zip(feature_names, sum_tfidf_scores))\n",
        "\n",
        "        sorted_keywords = sorted(keywords_with_scores, key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        top_keywords_df = pd.DataFrame(\n",
        "            [(k, round(s, 4)) for k, s in sorted_keywords[:top_n]],\n",
        "            columns=['é—œéµå­—', 'TF-IDFå¹³å‡æ¬Šé‡']\n",
        "        )\n",
        "\n",
        "        log_output.append(f\"âœ… æˆåŠŸæå– Top {len(top_keywords_df)} å€‹é—œéµå­—ã€‚\")\n",
        "\n",
        "        return top_keywords_df, log_output\n",
        "\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"âŒ TF-IDF åˆ†æç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "        return pd.DataFrame(columns=STATS_HEADER), log_output\n",
        "\n",
        "\n",
        "# --- 2.3 Gemini API ç”Ÿæˆæ‘˜è¦ (Model å‚³éå¼·åŒ–èˆ‡é‡è©¦æ©Ÿåˆ¶) ---\n",
        "def get_gemini_summary(keywords_df, log_output, model_obj):\n",
        "    \"\"\"æ¥æ”¶ model_obj ä½œç‚ºåƒæ•¸ï¼Œä¸¦æ–°å¢é‡è©¦æ©Ÿåˆ¶\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 4. Gemini æ‘˜è¦æ—¥èªŒ ---\")\n",
        "\n",
        "    if model_obj is None:\n",
        "        error_msg = \"âŒ Gemini æ¨¡å‹æœªåˆå§‹åŒ–ã€‚è«‹ç¢ºèª Colab Secret 'gemini' å·²è¨­å®šä¸”æˆæ¬ŠæˆåŠŸã€‚\"\n",
        "        log_output.append(error_msg)\n",
        "        return error_msg, log_output\n",
        "\n",
        "    if keywords_df.empty:\n",
        "        log_output.append(\"âš ï¸ ç¼ºå°‘é—œéµå­—ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦ã€‚\")\n",
        "        return \"âš ï¸ æ²’æœ‰é—œéµå­—ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦ã€‚\", log_output\n",
        "\n",
        "    keywords_list = keywords_df['é—œéµå­—'].tolist()\n",
        "    prompt = f\"\"\"\n",
        "    æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„è‚¡å¸‚æ•¸æ“šåˆ†æå¸«ã€‚\n",
        "\n",
        "    ä»»å‹™ï¼š\n",
        "    è«‹æ ¹æ“š Yahoo è‚¡å¸‚æ–°èçš„ {len(keywords_list)} å€‹ç†±é–€é—œéµå­—ï¼Œç”Ÿæˆä¸€ä»½å°ˆæ¥­çš„è‚¡å¸‚åˆ†æå ±å‘Šã€‚\n",
        "\n",
        "    ç†±é–€é—œéµå­— (ä¾ TF-IDF ç¸½æ¬Šé‡æ’åº)ï¼š\n",
        "    {', '.join(keywords_list)}\n",
        "\n",
        "    è¼¸å‡ºæ ¼å¼è¦æ±‚ (è«‹åš´æ ¼éµå®ˆ)ï¼š\n",
        "    1.  **äº”å¥æ´å¯Ÿæ‘˜è¦**ï¼šæ¢åˆ—å¼ï¼Œæ¯å¥éƒ½æ˜¯ç²¾é—¢çš„è‚¡å¸‚è§€å¯Ÿã€‚\n",
        "    2.  **ä¸€æ®µ 120 å­—çµè«–**ï¼šç¸½çµç›®å‰çš„è‚¡å¸‚è¶¨å‹¢æˆ–æŠ•è³‡æ©Ÿæœƒã€‚\n",
        "\n",
        "    è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\n",
        "    \"\"\"\n",
        "\n",
        "    summary_text = \"\"\n",
        "    MAX_RETRIES = 5 # ä¿æŒ 5 æ¬¡é‡è©¦\n",
        "    BASE_DELAY = 10\n",
        "    TIMEOUT_SECONDS = 180 # ä¿æŒ 180 ç§’è¶…æ™‚\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            log_output.append(f\"æ¨¡å‹è«‹æ±‚åƒæ•¸: gemini-2.5-flash, å˜—è©¦æ¬¡æ•¸: {attempt + 1}/{MAX_RETRIES}\")\n",
        "\n",
        "            # ä½¿ç”¨å‚³å…¥çš„ model_obj\n",
        "            response = model_obj.generate_content(prompt, request_options={\"timeout\": TIMEOUT_SECONDS})\n",
        "\n",
        "            summary_text = response.text.replace(\"#\", \"\").replace(\"*\", \"\")\n",
        "            log_output.append(\"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸã€‚\")\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"âŒ Gemini API å‘¼å«å¤±æ•— (å˜—è©¦ {attempt + 1}): {e}\"\n",
        "            log_output.append(error_msg)\n",
        "\n",
        "            if attempt < MAX_RETRIES - 1:\n",
        "                sleep_time = BASE_DELAY * (attempt + 1)\n",
        "                log_output.append(f\"â³ ç­‰å¾… {sleep_time} ç§’å¾Œé‡è©¦...\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                summary_text = error_msg\n",
        "                log_output.append(\"âŒ é‡è©¦æ¬¡æ•¸å·²ç”¨ç›¡ï¼Œæµç¨‹çµ‚æ­¢ã€‚\")\n",
        "\n",
        "    # --- å¯«å…¥ AI æ‘˜è¦è‡³ Sheet ---\n",
        "    global gsheets\n",
        "    if gsheets:\n",
        "        try:\n",
        "            ws_summary = get_or_create_worksheet(gsheets, \"AIæ‘˜è¦å ±å‘Š\")\n",
        "\n",
        "            new_row_df = pd.DataFrame([{\n",
        "                \"created_at\": dt.now(gettz(TIMEZONE)).isoformat(),\n",
        "                \"keywords_used\": \", \".join(keywords_list[:10]),\n",
        "                \"summary_report\": summary_text\n",
        "            }], columns=SUMMARY_HEADER)\n",
        "\n",
        "            df_existing = ws_summary.get_all_values()\n",
        "            df_existing_data = [row for row in df_existing if row != SUMMARY_HEADER]\n",
        "\n",
        "            new_data_row = new_row_df.iloc[0].values.tolist()\n",
        "\n",
        "            ws_summary.clear()\n",
        "            ws_summary.update([SUMMARY_HEADER] + [new_data_row] + df_existing_data, value_input_option=\"USER_ENTERED\")\n",
        "\n",
        "            log_output.append(\"âœ… æ‘˜è¦å’Œé—œéµè©å·²æˆåŠŸå¯«å…¥ 'AIæ‘˜è¦å ±å‘Š' å·¥ä½œè¡¨ã€‚\")\n",
        "        except Exception as e:\n",
        "            log_output.append(f\"âŒ å¯«å…¥ AI æ‘˜è¦è‡³ Sheet å¤±æ•—: {e}\")\n",
        "\n",
        "    return summary_text, log_output\n",
        "\n",
        "# ================================\n",
        "# 3. Gradio æ•´åˆå‡½å¼ (ä½¿ç”¨å–®æ¬¡å›å‚³)\n",
        "# ================================\n",
        "def run_full_automation_flow(top_n_str, articles_to_fetch_str):\n",
        "    \"\"\"Gradio é»æ“Šå¾ŒåŸ·è¡Œçš„å®Œæ•´æµç¨‹\"\"\"\n",
        "\n",
        "    global gsheets, gemini_model\n",
        "\n",
        "    empty_df = pd.DataFrame(columns=STATS_HEADER)\n",
        "    empty_scraped_df = pd.DataFrame(columns=CLIPS_HEADER)\n",
        "    empty_str = \"\"\n",
        "    log_output = []\n",
        "\n",
        "    SITE_NAME = \"Yahoo è‚¡å¸‚æ–°è\"\n",
        "    site_list = [SITE_NAME]\n",
        "\n",
        "    # --- åƒæ•¸åˆå§‹åŒ– (Gradio è¦æ±‚å›å‚³ 6 å€‹é …ç›®) ---\n",
        "    current_log = \"æ—¥èªŒå°‡é¡¯ç¤ºæ–¼æ­¤...\"\n",
        "    current_keywords = empty_df\n",
        "    current_summary = empty_str\n",
        "    current_plot_df = None\n",
        "    current_site_radio = gr.Radio(choices=[\"å°šæœªåŸ·è¡Œ\"], value=\"å°šæœªåŸ·è¡Œ\")\n",
        "    current_scraped_data = empty_scraped_df\n",
        "\n",
        "\n",
        "    # --- åƒæ•¸é©—è­‰ ---\n",
        "    try:\n",
        "        top_n = int(top_n_str)\n",
        "        articles_to_fetch = int(articles_to_fetch_str)\n",
        "        if top_n <= 0 or articles_to_fetch <= 0:\n",
        "            log_output.append(\"âŒ Top N æˆ–çˆ¬å–æ–‡ç« æ•¸å¿…é ˆæ˜¯å¤§æ–¼ 0 çš„æ•¸å­—ã€‚\")\n",
        "            current_log = \"\\n\".join(log_output)\n",
        "            return current_log, current_keywords, current_summary, current_plot_df, gr.Radio(choices=site_list), current_scraped_data\n",
        "    except ValueError:\n",
        "        log_output.append(\"âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—ã€‚\")\n",
        "        current_log = \"\\n\".join(log_output)\n",
        "        return current_log, current_keywords, current_summary, current_plot_df, gr.Radio(choices=site_list), current_scraped_data\n",
        "\n",
        "    # --- é—œéµæª¢æŸ¥ï¼šGemini Model æ˜¯å¦å°±ç·’ ---\n",
        "    if gemini_model is None:\n",
        "        log_output.append(\"âŒ æµç¨‹çµ‚æ­¢ï¼šGemini æ¨¡å‹æœªåˆå§‹åŒ–ã€‚è«‹ç¢ºä¿ Colab Secret 'gemini' å·²è¨­å®šä¸”æˆæ¬ŠæˆåŠŸã€‚\")\n",
        "        current_log = \"\\n\".join(log_output)\n",
        "        return current_log, current_keywords, current_summary, current_plot_df, gr.Radio(choices=site_list, value=SITE_NAME), current_scraped_data\n",
        "\n",
        "    # --- è‡ªå‹•åŒ–æµç¨‹ ---\n",
        "    log_output.append(\"===================================================\")\n",
        "    current_time_str = dt.now(gettz(TIMEZONE)).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    log_output.append(f\"ğŸš€ è‡ªå‹•åŒ–æµç¨‹å•Ÿå‹• ({current_time_str})\")\n",
        "    log_output.append(\"===================================================\")\n",
        "\n",
        "    try:\n",
        "        # --- æ­¥é©Ÿ 1: çˆ¬èŸ² (Yahoo News) ---\n",
        "        log_output.append(f\"1/4: ğŸƒâ€â™‚ï¸ é–‹å§‹çˆ¬å– {SITE_NAME} æ–‡ç« ï¼Œè«‹ç¨ç­‰...\")\n",
        "\n",
        "        scraped_df, log_output = scrape_yahoo_stock_news(articles_to_fetch, log_output)\n",
        "        display_df = scraped_df[[\"æ—¥æœŸ\", \"ä½œè€…\", \"æ¨™é¡Œ\", \"é€£çµ\", \"å…§æ–‡\"]]\n",
        "        current_scraped_data = display_df\n",
        "        current_site_radio = gr.Radio(choices=site_list, value=SITE_NAME)\n",
        "\n",
        "        if scraped_df.empty:\n",
        "            log_output.append(\"âŒ çˆ¬èŸ²å¤±æ•—ï¼ŒæœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™ã€‚æµç¨‹çµ‚æ­¢ã€‚\")\n",
        "            current_log = \"\\n\".join(log_output)\n",
        "            return current_log, current_keywords, current_summary, current_plot_df, current_site_radio, current_scraped_data\n",
        "\n",
        "        # --- æ­¥é©Ÿ 2: å¯«å…¥ Sheet (æ–‡ç« åˆ—è¡¨) ---\n",
        "        log_output = write_to_sheet(gsheets, \"Yahooæ–‡ç« åˆ—è¡¨\", scraped_df, log_output, CLIPS_HEADER)\n",
        "\n",
        "        # --- æ­¥é©Ÿ 3: TF-IDF åˆ†æ ---\n",
        "        log_output.append(\"2/4: ğŸ“Š æ­£åœ¨é€²è¡Œ Sklearn TF-IDF é—œéµå­—åˆ†æ...\")\n",
        "\n",
        "        keywords_df, log_output = get_tfidf_keywords(scraped_df, top_n, log_output)\n",
        "        current_keywords = keywords_df\n",
        "\n",
        "        if not keywords_df.empty:\n",
        "          current_plot_df = keywords_df.sort_values(\"TF-IDFå¹³å‡æ¬Šé‡\", ascending=True)\n",
        "\n",
        "        if keywords_df.empty:\n",
        "            log_output.append(\"âš ï¸ åˆ†æå®Œæˆï¼Œä½†æœªæå–åˆ°é—œéµå­—ã€‚æµç¨‹çµ‚æ­¢ã€‚\")\n",
        "            current_log = \"\\n\".join(log_output)\n",
        "            return current_log, current_keywords, current_summary, current_plot_df, current_site_radio, current_scraped_data\n",
        "\n",
        "        # --- æ­¥é©Ÿ 4: å¯«å…¥ Sheet (ç†±è©çµ±è¨ˆ) ---\n",
        "        log_output.append(\"3/4: ğŸ“ˆ æ­£åœ¨å°‡ Top ç†±è©å›å¯«è‡³ Sheet (ç†±è©çµ±è¨ˆ)...\")\n",
        "        log_output = write_to_sheet(gsheets, \"ç†±è©çµ±è¨ˆ\", keywords_df, log_output, STATS_HEADER)\n",
        "\n",
        "        # --- æ­¥é©Ÿ 5: Gemini æ‘˜è¦èˆ‡å¯«å…¥ Sheet (AIæ‘˜è¦å ±å‘Š) ---\n",
        "        log_output.append(\"4/4: ğŸ§  æ­£åœ¨å‘¼å« Gemini API ç”Ÿæˆæ‘˜è¦ (å·²è¨­å®š 180s è¶…æ™‚, 5 æ¬¡é‡è©¦)...\")\n",
        "\n",
        "        # å°‡ gemini_model ç‰©ä»¶ä½œç‚ºåƒæ•¸å‚³é\n",
        "        summary, log_output = get_gemini_summary(keywords_df, log_output, gemini_model)\n",
        "        current_summary = summary\n",
        "\n",
        "        # æœ€çµ‚å›å‚³\n",
        "        final_log = \"\\n\".join(log_output)\n",
        "        log_output.append(\"===================================================\")\n",
        "        log_output.append(\"âœ… å…¨éƒ¨æµç¨‹å®Œæˆï¼è«‹åˆ‡æ›åˆ°ã€Œæœ€çµ‚çµæœã€æ¨™ç±¤é æŸ¥çœ‹ã€‚\")\n",
        "\n",
        "        return final_log, current_keywords, current_summary, current_plot_df, current_site_radio, current_scraped_data\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ æµç¨‹ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ï¼š{e}\\n{traceback.format_exc()}\"\n",
        "        log_output.append(error_msg)\n",
        "        final_log = \"\\n\".join(log_output)\n",
        "        return final_log, empty_df, empty_str, None, gr.Radio(choices=site_list, value=SITE_NAME), empty_scraped_df\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4. å•Ÿå‹• Gradio ä»‹é¢ (ä½¿ç”¨æ©˜è‰²ä¸»é¡Œä¸¦å¼·åˆ¶å…¬é–‹åˆ†äº«)\n",
        "# ================================\n",
        "print(\"\\nğŸš€ æ­£åœ¨å•Ÿå‹• Gradio ä»‹é¢...\")\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"orange\"), title=\"Yahoo è‚¡å¸‚æ–°èåˆ†æèˆ‡ AI æ‘˜è¦ï¼ˆSheet å¼·åŒ–ç‰ˆï¼‰\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # ğŸ“ˆ Yahoo è‚¡å¸‚æ–°èåˆ†æ â†’ TF-IDF é—œéµè© â†’ AI æ´å¯Ÿæ‘˜è¦\n",
        "        æ­¤å·¥å…·æœƒè‡ªå‹•åŸ·è¡Œï¼š**Yahoo çˆ¬èŸ² â†’ å¯«å…¥ Sheet (æ–‡ç« åˆ—è¡¨) â†’ TF-IDF çµ±è¨ˆ â†’ å¯«å…¥ Sheet (ç†±è©çµ±è¨ˆ) â†’ Gemini ç”Ÿæˆæ‘˜è¦ â†’ å¯«å…¥ Sheet (AIæ‘˜è¦å ±å‘Š)**ã€‚\n",
        "        *ç‚ºè§£æ±ºè¶…æ™‚å•é¡Œï¼Œå·²å°‡ Gemini API è¶…æ™‚æé«˜è‡³ 180 ç§’ï¼Œä¸¦åŠ å…¥ 5 æ¬¡é‡è©¦æ©Ÿåˆ¶ã€‚è«‹åœ¨ Gradio æä¾›çš„**å…¬é–‹ç¶²å€** (Public URL) ä¸Šæ“ä½œï¼Œä»¥é¿å… Colab å…§åµŒä»‹é¢çš„é€£ç·šå•é¡Œã€‚*\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"ğŸš€ è‡ªå‹•åŒ–æµç¨‹åŸ·è¡Œ\"):\n",
        "        with gr.Row():\n",
        "            articles_to_fetch_input = gr.Textbox(label=\"è¦çˆ¬å–çš„æ–‡ç« æ•¸é‡ (Limit)\", value=\"10\", scale=1)\n",
        "            top_n_input = gr.Textbox(label=\"è¦çµ±è¨ˆçš„ Top N ç†±è©æ•¸é‡\", value=\"20\", scale=1)\n",
        "            run_btn = gr.Button(\"ğŸš€ ä¸€éµå•Ÿå‹• Yahoo è‚¡å¸‚æ–°èåˆ†æ\", variant=\"primary\", scale=2)\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        with gr.Row():\n",
        "            gr.Textbox(label=\"ç›®æ¨™ç¶²ç«™\", value=YAHOO_STOCK_URL, interactive=False, scale=1)\n",
        "            gr.Textbox(label=\"çˆ¬èŸ²æ¨¡å¼\", value=\"å°ˆé–€é‡å° Yahoo å…§é æ“·å–\", interactive=False, scale=1)\n",
        "\n",
        "        with gr.Tabs():\n",
        "\n",
        "            with gr.TabItem(\"ğŸ› ï¸ æŠ€è¡“æ—¥èªŒèˆ‡è¼¸å‡ºç´°ç¯€\"):\n",
        "                log_output_text = gr.Textbox(\n",
        "                    label=\"è©³ç´°æµç¨‹æ—¥èªŒ (çˆ¬èŸ²ã€å¯«å…¥ã€åˆ†ææ­¥é©Ÿ)\",\n",
        "                    lines=30,\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "            with gr.TabItem(\"ğŸ•¸ï¸ çˆ¬å–æ–‡ç« åˆ—è¡¨\"):\n",
        "                site_list_output = gr.Radio(\n",
        "                    label=\"è³‡æ–™ä¾†æº\",\n",
        "                    choices=[\"å°šæœªåŸ·è¡Œ\"],\n",
        "                    value=\"å°šæœªåŸ·è¡Œ\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                gr.Markdown(\"---\")\n",
        "\n",
        "                scraped_data_output = gr.Dataframe(\n",
        "                    label=\"çˆ¬å–æ–‡ç« åˆ—è¡¨ (åŸå§‹è³‡æ–™)\",\n",
        "                    headers=[\"æ—¥æœŸ\", \"ä½œè€…\", \"æ¨™é¡Œ\", \"é€£çµ\", \"å…§æ–‡\"],\n",
        "                    interactive=True,\n",
        "                    row_count=(15, 'dynamic')\n",
        "                )\n",
        "\n",
        "                link_display_output = gr.Markdown(\n",
        "          \t\t\tvalue=\"*åŸå§‹æ–‡ç« è³‡æ–™å·²é¡¯ç¤ºæ–¼è¡¨æ ¼ã€‚*\"\n",
        "          \t\t)\n",
        "\n",
        "            with gr.TabItem(\"âœ… æœ€çµ‚çµæœ\"):\n",
        "                summary_output = gr.Markdown(label=\"ğŸ¤– Gemini æ´å¯Ÿæ‘˜è¦èˆ‡çµè«–\")\n",
        "\n",
        "                keyword_plot_output = gr.BarPlot(\n",
        "                  label=\"ğŸ“ˆ Top N ç†±è©è¦–è¦ºåŒ–åœ–è¡¨\",\n",
        "                  x=\"TF-IDFå¹³å‡æ¬Šé‡\",\n",
        "                  y=\"é—œéµå­—\",\n",
        "                  tooltip=['é—œéµå­—', 'TF-IDFå¹³å‡æ¬Šé‡'],\n",
        "                  color=\"TF-IDFå¹³å‡æ¬Šé‡\",\n",
        "                  vertical=False,\n",
        "                  height=400\n",
        "                )\n",
        "\n",
        "                keywords_output = gr.Dataframe(label=\"ğŸ“ˆ Top N ç†±è©çµ±è¨ˆçµæœ (Sklearn TF-IDF ç¸½æ¬Šé‡)\")\n",
        "\n",
        "\n",
        "        # === ç¶å®šå‹•ä½œ ===\n",
        "        run_btn.click(\n",
        "          fn=run_full_automation_flow,\n",
        "          \tinputs=[top_n_input, articles_to_fetch_input],\n",
        "          \toutputs=[\n",
        "                log_output_text,\n",
        "                keywords_output,\n",
        "                summary_output,\n",
        "                keyword_plot_output,\n",
        "                site_list_output,\n",
        "                scraped_data_output\n",
        "            ]\n",
        "        )\n",
        "\n",
        "# ğŸ’¡ é—œéµä¿®æ­£ï¼šå¼·åˆ¶ä½¿ç”¨ share=True ä¾†ç”Ÿæˆå…¬é–‹ URL\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "swFHjZVnO39T",
        "outputId": "994b6c94-35c9-4f1e-ad05-c88c693c788a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Google Sheets æˆæ¬ŠæˆåŠŸã€‚\n",
            "âœ… Gemini API Key é…ç½®æˆåŠŸã€‚\n",
            "âœ… æˆåŠŸé–‹å•Ÿ Sheet: HW4_æ–‡å­—è³‡æ–™å°åˆ†æ\n",
            "\n",
            "ğŸš€ æ­£åœ¨å•Ÿå‹• Gradio ä»‹é¢...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e3f6210dade8186df3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e3f6210dade8186df3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e3f6210dade8186df3.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}